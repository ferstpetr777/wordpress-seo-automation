#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
integrated_web_researcher.py ‚Äî –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–∞–ª–æ–Ω–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ SEO-–∞–Ω–∞–ª–∏–∑–∞
"""

from __future__ import annotations
import json
import sqlite3
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from config.database_sqlite import DB_CONFIG
from modules.research.web_research_instruction import WebResearchInstruction

class IntegratedWebResearcher:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db_config = DB_CONFIG.get_config_dict()
        self.db_path = self.db_config['database']
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
        self.instruction = WebResearchInstruction()
        self.standard_instruction = self.instruction.get_instruction()
        
        self.logger.info("‚úÖ Integrated Web Researcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def execute_web_search(self, query: str) -> List[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–µ–±-–ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ web_search()
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–º—É–ª—è—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            
            mock_results = [
                {
                    "title": f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞: {query}",
                    "url": f"https://example.com/{query.replace(' ', '-')}",
                    "snippet": f"–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {query}. –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è 2025 –≥–æ–¥–∞.",
                    "domain": "example.com",
                    "date": "2025-10-14"
                },
                {
                    "title": f"–ì–∏–¥ –ø–æ {query} - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    "url": f"https://guide.com/{query.replace(' ', '-')}",
                    "snippet": f"–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {query}. –í–∫–ª—é—á–∞–µ—Ç –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã, –ø—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã.",
                    "domain": "guide.com",
                    "date": "2025-10-14"
                },
                {
                    "title": f"{query}: —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Å—Ä–æ–∫–∏, –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                    "url": f"https://info.com/{query.replace(' ', '-')}",
                    "snippet": f"–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {query}. –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ 2025 –≥–æ–¥—É.",
                    "domain": "info.com",
                    "date": "2025-10-14"
                }
            ]
            
            self.logger.info(f"‚úÖ –í–µ–±-–ø–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω: {len(mock_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{query}'")
            return mock_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def analyze_serp_results(self, results: List[Dict[str, Any]], keyword: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SERP —Å–æ–≥–ª–∞—Å–Ω–æ —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            serp_analysis = {
                "total_results": len(results),
                "analysis_date": datetime.now().isoformat(),
                "keyword": keyword,
                "top_results": []
            }
            
            for i, result in enumerate(results, 1):
                serp_item = {
                    "position": i,
                    "url": result["url"],
                    "type": self._classify_content_type(result["title"]),
                    "date": result["date"],
                    "length": self._estimate_content_length(result["snippet"]),
                    "h1": result["title"],
                    "features": self._identify_features(result["snippet"]),
                    "domain": result["domain"]
                }
                serp_analysis["top_results"].append(serp_item)
            
            self.logger.info(f"‚úÖ SERP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(serp_analysis['top_results'])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return serp_analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ SERP –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"error": str(e)}
    
    def build_semantic_clusters(self, keyword: str, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            clusters = {
                "primary": {
                    "cluster": "–û—Å–Ω–æ–≤–Ω–æ–π",
                    "keywords": [keyword, keyword.replace(" ", "")],
                    "intent": "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π",
                    "example_h2": f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {keyword}",
                    "notes": "–í—ã—Å–æ–∫–∏–π –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"
                },
                "supporting": {
                    "cluster": "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π", 
                    "keywords": [
                        f"{keyword} —É—Å–ª–æ–≤–∏—è",
                        f"{keyword} —Ç–∞—Ä–∏—Ñ—ã",
                        f"{keyword} –¥–æ–∫—É–º–µ–Ω—Ç—ã"
                    ],
                    "intent": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π",
                    "example_h2": "–£—Å–ª–æ–≤–∏—è –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è",
                    "notes": "–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                },
                "questions": {
                    "cluster": "–í–æ–ø—Ä–æ—Å—ã",
                    "keywords": [
                        f"–∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å {keyword}",
                        f"—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç {keyword}",
                        f"–∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è {keyword}"
                    ],
                    "intent": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π",
                    "example_h2": "–ß–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã",
                    "notes": "FAQ –±–ª–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω"
                }
            }
            
            self.logger.info(f"‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã")
            return clusters
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
            return {}
    
    def identify_content_gaps(self, keyword: str, serp_data: Dict[str, Any]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç-–≥—ç–ø–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            gaps = [
                f"–°–ø–µ—Ü–∏—Ñ–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å {keyword}",
                f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∞—Ä–∏—Ñ–æ–≤ –ø–æ {keyword}",
                f"–ö–µ–π—Å—ã –ø–æ–ª—É—á–µ–Ω–∏—è {keyword}",
                f"–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ú–°–ë –ø–æ {keyword}",
                f"–¶–∏—Ñ—Ä–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è {keyword}",
                f"–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å {keyword}"
            ]
            
            self.logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç-–≥—ç–ø—ã –≤—ã—è–≤–ª–µ–Ω—ã: {len(gaps)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            return gaps
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—è–≤–ª–µ–Ω–∏—è –≥—ç–ø–æ–≤: {e}")
            return []
    
    def collect_facts_and_figures(self, keyword: str) -> List[Dict[str, str]]:
        """–°–±–æ—Ä —Ü–∏—Ñ—Ä –∏ —Ñ–∞–∫—Ç–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            facts = [
                {
                    "fact": "–°—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å —Å—Ä–æ–∫ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –º–∏–Ω–∏–º—É–º –Ω–∞ 1 –º–µ—Å—è—Ü",
                    "source": "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ44-–§–ó, —Å—Ç. 96",
                    "date": "2025-10-14",
                    "url": "https://www.consultant.ru/document/cons_doc_LAW_144624/"
                },
                {
                    "fact": "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∑–∞—è–≤–∫–∏ - 2 –º–µ—Å—è—Ü–∞",
                    "source": "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ44-–§–ó",
                    "date": "2025-10-14", 
                    "url": "https://www.consultant.ru/document/cons_doc_LAW_144624/"
                }
            ]
            
            self.logger.info(f"‚úÖ –§–∞–∫—Ç—ã –∏ —Ü–∏—Ñ—Ä—ã —Å–æ–±—Ä–∞–Ω—ã: {len(facts)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            return facts
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Ñ–∞–∫—Ç–æ–≤: {e}")
            return []
    
    def generate_seo_elements(self, keyword: str) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            seo_elements = {
                "title_variants": [
                    f"{keyword}: —É—Å–ª–æ–≤–∏—è 2025",
                    f"{keyword}: —Ç–∞—Ä–∏—Ñ—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                    f"–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å {keyword}: –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    f"{keyword} –¥–ª—è –±–∏–∑–Ω–µ—Å–∞",
                    f"{keyword}: —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Å—Ä–æ–∫–∏, –¥–æ–∫—É–º–µ–Ω—Ç—ã"
                ],
                "meta_descriptions": [
                    f"{keyword}: —É—Å–ª–æ–≤–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è, —Ç–∞—Ä–∏—Ñ—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã. –ë—ã—Å—Ç—Ä–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –¥–ª—è –ú–°–ë.",
                    f"–£–∑–Ω–∞–π—Ç–µ, –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å {keyword}. –£—Å–ª–æ–≤–∏—è, —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Å—Ä–æ–∫–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è.",
                    f"{keyword}: –≤—ã–≥–æ–¥–Ω—ã–µ —Ç–∞—Ä–∏—Ñ—ã, –±—ã—Å—Ç—Ä–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ. –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
                ],
                "og_title": f"{keyword}: —É—Å–ª–æ–≤–∏—è –∏ —Ç–∞—Ä–∏—Ñ—ã 2025",
                "og_description": f"–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {keyword}. –£—Å–ª–æ–≤–∏—è, —Ç–∞—Ä–∏—Ñ—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å—Ä–æ–∫–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è."
            }
            
            self.logger.info(f"‚úÖ SEO —ç–ª–µ–º–µ–Ω—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
            return seo_elements
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO: {e}")
            return {}
    
    def generate_faq(self, keyword: str) -> List[Dict[str, str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è FAQ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            faq = [
                {
                    "question": f"–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è {keyword}?",
                    "answer": "–£—Å—Ç–∞–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å, –¥–æ–≥–æ–≤–æ—Ä –∏ –∑–∞—è–≤–∫–∞.",
                    "source_needed": True
                },
                {
                    "question": f"–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç {keyword}?",
                    "answer": "–°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—É–º–º—ã –∏ —Å—Ä–æ–∫–∞. –û–±—ã—á–Ω–æ –æ—Ç 0.5% –¥–æ 3% –≥–æ–¥–æ–≤—ã—Ö.",
                    "source_needed": True
                },
                {
                    "question": f"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è {keyword}?",
                    "answer": "–û—Ç 1 –¥–æ 5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –∫–æ–º–ø–ª–µ–∫—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
                    "source_needed": True
                }
            ]
            
            self.logger.info(f"‚úÖ FAQ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(faq)} –≤–æ–ø—Ä–æ—Å–æ–≤")
            return faq
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ FAQ: {e}")
            return []
    
    def create_kpi_brief(self, keyword: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ KPI –±—Ä–∏—Ñ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            kpi_brief = {
                "goal": f"–ò–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ø–æ–ª—É—á–µ–Ω–∏—è {keyword}",
                "intent": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π + –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π",
                "volume": "2500-3000 —Å–ª–æ–≤",
                "key_clusters": ["–û—Å–Ω–æ–≤–Ω–æ–π", "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π", "–í–æ–ø—Ä–æ—Å—ã"],
                "eeat_requirements": [
                    "–ê–≤—Ç–æ—Ä: —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∞–Ω–∫–æ–≤—Å–∫–∏–º –≥–∞—Ä–∞–Ω—Ç–∏—è–º",
                    "–°—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏",
                    "–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –¥–∞—Ç–∞–º–∏"
                ],
                "media_plan": [
                    "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä",
                    "–ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è",
                    "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ç–∞—Ä–∏—Ñ–æ–≤"
                ],
                "cta": [
                    "–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é",
                    "–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å",
                    "–ó–∞–∫–∞–∑–∞—Ç—å –∑–≤–æ–Ω–æ–∫"
                ],
                "deadlines": "2025-10-21"
            }
            
            self.logger.info(f"‚úÖ KPI –±—Ä–∏—Ñ —Å–æ–∑–¥–∞–Ω")
            return kpi_brief
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è KPI: {e}")
            return {}
    
    def run_full_analysis(self, keyword: str, brand_domain: str = None) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        start_time = datetime.now()
        
        try:
            print(f"üîç –ó–ê–ü–£–°–ö –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê: {keyword}")
            print("=" * 70)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            search_queries = self.instruction.create_search_queries(keyword, brand_domain)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫–∏
            all_results = []
            for query_info in search_queries:
                print(f"üåê –ü–æ–∏—Å–∫: {query_info['query']}")
                results = self.execute_web_search(query_info['query'])
                all_results.extend(results)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            serp_analysis = self.analyze_serp_results(all_results[:5], keyword)
            semantic_clusters = self.build_semantic_clusters(keyword, serp_analysis)
            content_gaps = self.identify_content_gaps(keyword, serp_analysis)
            facts_figures = self.collect_facts_and_figures(keyword)
            seo_elements = self.generate_seo_elements(keyword)
            faq = self.generate_faq(keyword)
            kpi_brief = self.create_kpi_brief(keyword)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            full_analysis = {
                "research_id": f"integrated_{int(datetime.now().timestamp())}",
                "keyword": keyword,
                "research_name": f"–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {keyword}",
                "created_at": datetime.now().isoformat(),
                "execution_time_seconds": execution_time,
                "instruction_version": self.standard_instruction["version"],
                "search_queries_used": [q["query"] for q in search_queries],
                "serp_analysis": serp_analysis,
                "semantic_clusters": semantic_clusters,
                "content_gaps": content_gaps,
                "facts_and_figures": facts_figures,
                "seo_elements": seo_elements,
                "faq": faq,
                "kpi_brief": kpi_brief,
                "status": "completed"
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            analysis_id = self.save_analysis_to_db(full_analysis)
            full_analysis["database_id"] = analysis_id
            
            print(f"‚úÖ –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
            print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"   ID –≤ –ë–î: {analysis_id}")
            print(f"   –ü–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(search_queries)}")
            print(f"   SERP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(serp_analysis.get('top_results', []))}")
            print(f"   FAQ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(faq)}")
            
            return full_analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            raise
    
    def save_analysis_to_db(self, analysis_data: Dict[str, Any]) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS integrated_web_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    research_id TEXT UNIQUE NOT NULL,
                    keyword TEXT NOT NULL,
                    research_name TEXT NOT NULL,
                    analysis_data TEXT NOT NULL,
                    instruction_version TEXT NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    execution_time_seconds REAL,
                    status TEXT DEFAULT 'completed'
                )
            ''')
            
            insert_query = '''
                INSERT INTO integrated_web_analysis 
                (research_id, keyword, research_name, analysis_data, instruction_version, execution_time_seconds, status)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            '''
            
            values = (
                analysis_data["research_id"],
                analysis_data["keyword"],
                analysis_data["research_name"],
                json.dumps(analysis_data, ensure_ascii=False),
                analysis_data["instruction_version"],
                analysis_data["execution_time_seconds"],
                analysis_data["status"]
            )
            
            cursor.execute(insert_query, values)
            analysis_id = cursor.lastrowid
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î (ID: {analysis_id})")
            return analysis_id
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
            if 'conn' in locals():
                conn.rollback()
                conn.close()
            raise
    
    def _classify_content_type(self, title: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        title_lower = title.lower()
        if "–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä" in title_lower or "—Ä–∞—Å—á–µ—Ç" in title_lower:
            return "–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä"
        elif "faq" in title_lower or "–≤–æ–ø—Ä–æ—Å" in title_lower:
            return "FAQ"
        elif "–≥–∏–¥" in title_lower or "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ" in title_lower:
            return "–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
        else:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è"
    
    def _estimate_content_length(self, snippet: str) -> str:
        """–û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        word_count = len(snippet.split())
        if word_count < 100:
            return "~500 —Å–ª–æ–≤"
        elif word_count < 200:
            return "~1000 —Å–ª–æ–≤"
        else:
            return "~1500+ —Å–ª–æ–≤"
    
    def _identify_features(self, snippet: str) -> str:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        features = []
        snippet_lower = snippet.lower()
        
        if "—Ç–∞–±–ª–∏—Ü–∞" in snippet_lower:
            features.append("–¢–∞–±–ª–∏—Ü—ã")
        if "–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä" in snippet_lower:
            features.append("–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä")
        if "–ø—Ä–∏–º–µ—Ä" in snippet_lower:
            features.append("–ü—Ä–∏–º–µ—Ä—ã")
        if "–∫–µ–π—Å" in snippet_lower:
            features.append("–ö–µ–π—Å—ã")
        
        return ", ".join(features) if features else "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å")
    parser.add_argument("--kw", required=True, help="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--brand", help="–î–æ–º–µ–Ω –±—Ä–µ–Ω–¥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    researcher = IntegratedWebResearcher()
    
    try:
        analysis = researcher.run_full_analysis(args.kw, args.brand)
        
        print(f"\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {analysis['keyword']}")
        print(f"   ID –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {analysis['research_id']}")
        print(f"   ID –≤ –ë–î: {analysis['database_id']}")
        print(f"   –°—Ç–∞—Ç—É—Å: {analysis['status']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
