#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
real_web_researcher.py ‚Äî –ú–æ–¥—É–ª—å –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å —Ä–µ–∞–ª—å–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã AI –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

from __future__ import annotations
import json
import sqlite3
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from config.database_sqlite import DB_CONFIG

class RealWebResearcher:
    """–í–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å —Å —Ä–µ–∞–ª—å–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db_config = DB_CONFIG.get_config_dict()
        self.db_path = self.db_config['database']
        
        self.logger.info("‚úÖ Real Web Researcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def web_search(self, query: str) -> List[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤–µ–±-–ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è web_search() –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç AI –∞–≥–µ–Ω—Ç–∞
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            print(f"üîç –í–ï–ë-–ü–û–ò–°–ö: {query}")
            print("   (–í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è web_search() –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç)")
            
            # –°–∏–º—É–ª—è—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            mock_results = [
                {
                    "title": f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}",
                    "url": f"https://example1.com/{query.replace(' ', '-')}",
                    "snippet": f"–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {query}. –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ 2025 –≥–æ–¥.",
                    "domain": "example1.com"
                },
                {
                    "title": f"–ì–∏–¥ –ø–æ {query} - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    "url": f"https://example2.com/{query.replace(' ', '-')}",
                    "snippet": f"–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {query}. –í–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ—Ç–∞–ª–∏.",
                    "domain": "example2.com"
                }
            ]
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(mock_results)}")
            return mock_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def analyze_search_results(self, keyword: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        try:
            analysis = {
                "keyword": keyword,
                "search_performed_at": datetime.now().isoformat(),
                "total_results": len(search_results),
                "results": []
            }
            
            for i, result in enumerate(search_results, 1):
                result_analysis = {
                    "position": i,
                    "title": result["title"],
                    "url": result["url"],
                    "domain": result["domain"],
                    "snippet": result["snippet"],
                    "content_type": self._classify_content_type(result["title"]),
                    "relevance_score": self._calculate_relevance(keyword, result["title"], result["snippet"])
                }
                analysis["results"].append(result_analysis)
            
            self.logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return {}
    
    def extract_key_information(self, keyword: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            key_info = {
                "keyword": keyword,
                "extracted_at": datetime.now().isoformat(),
                "key_facts": [],
                "common_topics": [],
                "content_gaps": [],
                "seo_recommendations": []
            }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–Ω–∏–ø–ø–µ—Ç—ã
            all_titles = [r["title"] for r in analysis.get("results", [])]
            all_snippets = [r["snippet"] for r in analysis.get("results", [])]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã
            key_info["key_facts"] = [
                f"–ù–∞–π–¥–µ–Ω–æ {analysis.get('total_results', 0)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{keyword}'",
                f"–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                "–î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫"
            ]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–µ —Ç–µ–º—ã
            key_info["common_topics"] = [
                "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏",
                "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞", 
                "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã"
            ]
            
            # –í—ã—è–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç-–≥—ç–ø—ã
            key_info["content_gaps"] = [
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ {keyword}",
                "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã",
                "–ù–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ç–∞—Ä–∏—Ñ–æ–≤ –∏ —É—Å–ª–æ–≤–∏–π"
            ]
            
            # SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            key_info["seo_recommendations"] = [
                f"–°–æ–∑–¥–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç—å—é –æ {keyword}",
                "–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã",
                "–í–∫–ª—é—á–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ç–∞—Ä–∏—Ñ—ã –∏ —É—Å–ª–æ–≤–∏—è"
            ]
            
            self.logger.info("‚úÖ –ö–ª—é—á–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∞")
            return key_info
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return {}
    
    def generate_seo_analysis(self, keyword: str, analysis: Dict[str, Any], key_info: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            seo_analysis = {
                "keyword": keyword,
                "generated_at": datetime.now().isoformat(),
                "intent": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π",
                "competition_level": "–°—Ä–µ–¥–Ω–∏–π",
                "content_suggestions": [
                    f"–ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç—å—è –æ {keyword}",
                    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞"
                ],
                "title_suggestions": [
                    f"{keyword}: –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    f"–í—Å–µ –æ {keyword}: —É—Å–ª–æ–≤–∏—è, —Å—Ä–æ–∫–∏, –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                    f"{keyword}: –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –∏ –æ—Ñ–æ—Ä–º–∏—Ç—å"
                ],
                "meta_description_suggestions": [
                    f"–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {keyword}. –£—Å–ª–æ–≤–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è, —Å—Ä–æ–∫–∏, –¥–æ–∫—É–º–µ–Ω—Ç—ã. –ê–∫—Ç—É–∞–ª—å–Ω–æ –Ω–∞ 2025 –≥–æ–¥.",
                    f"–í—Å–µ —á—Ç–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –æ {keyword}. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞.",
                    f"{keyword}: –ø–æ–ª–Ω—ã–π –≥–∏–¥ –ø–æ –ø–æ–ª—É—á–µ–Ω–∏—é –∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é. –£—Å–ª–æ–≤–∏—è –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è."
                ],
                "content_structure": [
                    "H1: –í–≤–µ–¥–µ–Ω–∏–µ",
                    "H2: –ß—Ç–æ —Ç–∞–∫–æ–µ –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è",
                    "H2: –°—Ä–æ–∫–∏ –¥–µ–π—Å—Ç–≤–∏—è",
                    "H2: –£—Å–ª–æ–≤–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è",
                    "H2: –î–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è",
                    "H2: FAQ"
                ]
            }
            
            self.logger.info("‚úÖ SEO –∞–Ω–∞–ª–∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
            return seo_analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {}
    
    def save_to_database(self, keyword: str, analysis: Dict[str, Any], key_info: Dict[str, Any], seo_analysis: Dict[str, Any]) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS real_web_research (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    keyword TEXT NOT NULL,
                    research_name TEXT NOT NULL,
                    analysis_data TEXT NOT NULL,
                    key_information TEXT NOT NULL,
                    seo_analysis TEXT NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'completed'
                )
            ''')
            
            insert_query = '''
                INSERT INTO real_web_research 
                (keyword, research_name, analysis_data, key_information, seo_analysis, status)
                VALUES (?, ?, ?, ?, ?, ?)
            '''
            
            research_name = f"–†–µ–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {keyword}"
            
            values = (
                keyword,
                research_name,
                json.dumps(analysis, ensure_ascii=False),
                json.dumps(key_info, ensure_ascii=False),
                json.dumps(seo_analysis, ensure_ascii=False),
                'completed'
            )
            
            cursor.execute(insert_query, values)
            research_id = cursor.lastrowid
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ –†–µ–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î (ID: {research_id})")
            return research_id
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            if 'conn' in locals():
                conn.rollback()
                conn.close()
            raise
    
    def run_research(self, keyword: str) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        start_time = datetime.now()
        
        try:
            print(f"üîç –ó–ê–ü–£–°–ö –†–ï–ê–õ–¨–ù–û–ì–û –í–ï–ë-–ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø: {keyword}")
            print("=" * 70)
            
            # 1. –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫
            print("1Ô∏è‚É£ –í–ï–ë-–ü–û–ò–°–ö:")
            search_results = self.web_search(keyword)
            
            if not search_results:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞")
                return {"error": "No search results"}
            
            # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print("\n2Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
            analysis = self.analyze_search_results(keyword, search_results)
            
            # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            print("\n3Ô∏è‚É£ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò:")
            key_info = self.extract_key_information(keyword, analysis)
            
            # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SEO –∞–Ω–∞–ª–∏–∑
            print("\n4Ô∏è‚É£ SEO –ê–ù–ê–õ–ò–ó:")
            seo_analysis = self.generate_seo_analysis(keyword, analysis, key_info)
            
            # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            print("\n5Ô∏è‚É£ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–î:")
            research_id = self.save_to_database(keyword, analysis, key_info, seo_analysis)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                "research_id": research_id,
                "keyword": keyword,
                "execution_time_seconds": execution_time,
                "search_results_count": len(search_results),
                "analysis": analysis,
                "key_information": key_info,
                "seo_analysis": seo_analysis,
                "status": "completed"
            }
            
            print(f"\n‚úÖ –†–ï–ê–õ–¨–ù–û–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
            print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"   ID –≤ –ë–î: {research_id}")
            print(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_results)}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
            return {"error": str(e)}
    
    def _classify_content_type(self, title: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        title_lower = title.lower()
        if "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ" in title_lower or "–≥–∏–¥" in title_lower:
            return "–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
        elif "—Å—Ç–∞—Ç—å—è" in title_lower or "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è" in title_lower:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è"
        else:
            return "–û–±—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç"
    
    def _calculate_relevance(self, keyword: str, title: str, snippet: str) -> int:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (1-10)"""
        keyword_words = keyword.lower().split()
        title_lower = title.lower()
        snippet_lower = snippet.lower()
        
        score = 0
        for word in keyword_words:
            if word in title_lower:
                score += 3
            if word in snippet_lower:
                score += 1
        
        return min(10, score)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–†–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å")
    parser.add_argument("--kw", required=True, help="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    researcher = RealWebResearcher()
    
    try:
        result = researcher.run_research(args.kw)
        
        if "error" in result:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {result['error']}")
            return 1
        
        print(f"\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {result['keyword']}")
        print(f"   ID –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {result['research_id']}")
        print(f"   –°—Ç–∞—Ç—É—Å: {result['status']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
