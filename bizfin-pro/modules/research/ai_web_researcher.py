#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ai_web_researcher.py ‚Äî –ú–æ–¥—É–ª—å –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å AI –∞–≥–µ–Ω—Ç–æ–º
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ—Å—Ç—É–ø AI –∞–≥–µ–Ω—Ç–∞ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

from __future__ import annotations
import json
import re
import time
import logging
import sqlite3
import pickle
from collections import Counter
from dataclasses import dataclass
from datetime import datetime, timedelta, date
from typing import List, Optional, Dict, Literal, Any
from urllib.parse import urlparse, urljoin, quote_plus
from pathlib import Path

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from config.database_sqlite import DB_CONFIG

# ---------------------------
# Pydantic-–º–æ–¥–µ–ª–∏
# ---------------------------

ContentType = Literal["guide","FAQ","case","law_review","calculator","landing","news"]

class SerpItem:
    def __init__(self, rank: int, url: str, title: str, publisher: str = None, 
                 snippet: str = None, publish_date: date = None, 
                 content_type: ContentType = "guide", why_selected: str = None):
        self.rank = rank
        self.url = url
        self.title = title
        self.publisher = publisher or urlparse(url).netloc
        self.snippet = snippet
        self.publish_date = publish_date
        self.content_type = content_type
        self.why_selected = why_selected or "–¢–û–ü-—Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"
    
    def model_dump(self):
        return {
            "rank": self.rank,
            "url": self.url,
            "title": self.title,
            "publisher": self.publisher,
            "snippet": self.snippet,
            "publish_date": self.publish_date.isoformat() if self.publish_date else None,
            "content_type": self.content_type,
            "why_selected": self.why_selected
        }

class PageArtifact:
    def __init__(self, url: str, title: str, h_outline: List[str] = None,
                 content_plain: str = "", tables_tsv: List[str] = None,
                 faq: List[Dict[str, str]] = None, calculators: List[Dict[str, str]] = None,
                 legal_refs: List[str] = None, author: str = None, publisher: str = None,
                 publish_date: date = None, update_date: date = None,
                 schema_types: List[str] = None, ctas: List[str] = None,
                 reading_time_min: int = 0, word_count: int = 0):
        self.url = url
        self.title = title
        self.h_outline = h_outline or []
        self.content_plain = content_plain
        self.tables_tsv = tables_tsv or []
        self.faq = faq or []
        self.calculators = calculators or []
        self.legal_refs = legal_refs or []
        self.author = author
        self.publisher = publisher or urlparse(url).netloc
        self.publish_date = publish_date
        self.update_date = update_date
        self.schema_types = schema_types or []
        self.ctas = ctas or []
        self.reading_time_min = reading_time_min
        self.word_count = word_count
    
    def model_dump(self):
        return {
            "url": self.url,
            "title": self.title,
            "h_outline": self.h_outline,
            "content_plain": self.content_plain,
            "tables_tsv": self.tables_tsv,
            "faq": self.faq,
            "calculators": self.calculators,
            "legal_refs": self.legal_refs,
            "author": self.author,
            "publisher": self.publisher,
            "publish_date": self.publish_date.isoformat() if self.publish_date else None,
            "update_date": self.update_date.isoformat() if self.update_date else None,
            "schema_types": self.schema_types,
            "ctas": self.ctas,
            "reading_time_min": self.reading_time_min,
            "word_count": self.word_count
        }

class CorpusSynthesis:
    def __init__(self, consensus: List[Dict] = None, disagreements: List[str] = None,
                 legal_anchors: List[Dict] = None, common_outline: List[str] = None,
                 must_have_blocks: List[str] = None, entities: Dict[str, List[str]] = None,
                 risk_compliance: List[str] = None, freshness: List[str] = None):
        self.consensus = consensus or []
        self.disagreements = disagreements or []
        self.legal_anchors = legal_anchors or []
        self.common_outline = common_outline or []
        self.must_have_blocks = must_have_blocks or []
        self.entities = entities or {}
        self.risk_compliance = risk_compliance or []
        self.freshness = freshness or []
    
    def model_dump(self):
        return {
            "consensus": self.consensus,
            "disagreements": self.disagreements,
            "legal_anchors": self.legal_anchors,
            "common_outline": self.common_outline,
            "must_have_blocks": self.must_have_blocks,
            "entities": self.entities,
            "risk_compliance": self.risk_compliance,
            "freshness": self.freshness
        }

class SeoBlueprint:
    def __init__(self, title: str, h1: str, slug: str, meta_description: str,
                 outline: List[str] = None, blocks: List[str] = None,
                 faq: List[Dict[str, str]] = None, internal_links: List[Dict[str, str]] = None,
                 eeat: List[str] = None, tech: List[str] = None, schema: List[str] = None):
        self.title = title
        self.h1 = h1
        self.slug = slug
        self.meta_description = meta_description
        self.outline = outline or []
        self.blocks = blocks or []
        self.faq = faq or []
        self.internal_links = internal_links or []
        self.eeat = eeat or []
        self.tech = tech or []
        self.schema = schema or []
    
    def model_dump(self):
        return {
            "title": self.title,
            "h1": self.h1,
            "slug": self.slug,
            "meta_description": self.meta_description,
            "outline": self.outline,
            "blocks": self.blocks,
            "faq": self.faq,
            "internal_links": self.internal_links,
            "eeat": self.eeat,
            "tech": self.tech,
            "schema": self.schema
        }

# ---------------------------
# AI Web Researcher
# ---------------------------

class AIWebResearcher:
    """–í–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ AI –∞–≥–µ–Ω—Ç—É"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db_config = DB_CONFIG.get_config_dict()
        self.db_path = self.db_config['database']
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ë–î –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        self.logger.info("‚úÖ AI Web Researcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def search_via_ai_agent(self, query: str, max_results: int = 5) -> List[SerpItem]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞"""
        try:
            # –ò–º–∏—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞
            # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ AI –∞–≥–µ–Ω—Ç–∞
            mock_results = [
                {
                    "title": f"–†–µ–∑—É–ª—å—Ç–∞—Ç 1 –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'",
                    "url": f"https://example1.com/{query.replace(' ', '-')}",
                    "snippet": f"–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {query}. –°–æ–¥–µ—Ä–∂–∏—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."
                },
                {
                    "title": f"–ì–∏–¥ –ø–æ {query} - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    "url": f"https://example2.com/guide/{query.replace(' ', '-')}",
                    "snippet": f"–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {query}. –í–∫–ª—é—á–∞–µ—Ç –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã –∏ –ø—Ä–∏–º–µ—Ä—ã."
                },
                {
                    "title": f"{query}: —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Å—Ä–æ–∫–∏, –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                    "url": f"https://example3.com/info/{query.replace(' ', '-')}",
                    "snippet": f"–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {query}. –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ 2025 –≥–æ–¥—É."
                },
                {
                    "title": f"FAQ: –ß–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ {query}",
                    "url": f"https://example4.com/faq/{query.replace(' ', '-')}",
                    "snippet": f"–û—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ {query}. –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."
                },
                {
                    "title": f"–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä {query} –æ–Ω–ª–∞–π–Ω",
                    "url": f"https://example5.com/calc/{query.replace(' ', '-')}",
                    "snippet": f"–û–Ω–ª–∞–π–Ω –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ {query}. –ë—ã—Å—Ç—Ä–æ –∏ —Ç–æ—á–Ω–æ."
                }
            ]
            
            serp_items = []
            for i, result in enumerate(mock_results[:max_results], 1):
                serp_item = SerpItem(
                    rank=i,
                    url=result["url"],
                    title=result["title"],
                    snippet=result["snippet"],
                    why_selected=f"AI –ø–æ–∏—Å–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç #{i}"
                )
                serp_items.append(serp_item)
            
            self.logger.info(f"‚úÖ AI –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(serp_items)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return serp_items
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def analyze_page_via_ai(self, url: str, title: str) -> PageArtifact:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞"""
        try:
            domain = urlparse(url).netloc
            
            # –ò–º–∏—Ç–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞
            artifact = PageArtifact(
                url=url,
                title=title,
                h_outline=[
                    f"H1: {title}",
                    f"H2: –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏",
                    f"H2: –°—Ç–æ–∏–º–æ—Å—Ç—å –∏ —Ç–∞—Ä–∏—Ñ—ã",
                    f"H2: –°—Ä–æ–∫–∏ –∏ —É—Å–ª–æ–≤–∏—è",
                    f"H2: –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                    f"H2: –ü—Ä–æ—Ü–µ—Å—Å –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è",
                    f"H2: –ß–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã"
                ],
                content_plain=f"–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {title.lower()}. –í–∫–ª—é—á–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏, —Å—Ä–æ–∫–∞—Ö, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö. –ú–∞—Ç–µ—Ä–∏–∞–ª –æ–±–Ω–æ–≤–ª–µ–Ω –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –†–§.",
                faq=[
                    {"q": "–ö–∞–∫–æ–≤–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥–∏?", "a": "–°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—É–º–º—ã –∏ —Å—Ä–æ–∫–∞. –£—Ç–æ—á–Ω–∏—Ç–µ —É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞."},
                    {"q": "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?", "a": "–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤–∫–ª—é—á–∞–µ—Ç —É—Å—Ç–∞–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å –∏ –¥–æ–≥–æ–≤–æ—Ä."},
                    {"q": "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ?", "a": "–û–±—ã—á–Ω–æ –æ—Ç 1 –¥–æ 5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –±–∞–Ω–∫–∞."}
                ],
                legal_refs=["44-–§–ó", "223-–§–ó", "–ì–ö –†–§ —Å—Ç. 368"],
                ctas=["–ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—á–µ—Ç", "–û—Å—Ç–∞–≤–∏—Ç—å –∑–∞—è–≤–∫—É", "–ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è"],
                reading_time_min=3,
                word_count=150,
                schema_types=["Article", "FAQPage"],
                publisher=domain
            )
            
            self.logger.info(f"‚úÖ AI –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–≤–µ—Ä—à–µ–Ω: {domain}")
            return artifact
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None
    
    def synthesize_corpus(self, keyword: str, pages: List[PageArtifact]) -> CorpusSynthesis:
        """–°–∏–Ω—Ç–µ–∑ –∫–æ—Ä–ø—É—Å–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            h2_list = []
            for page in pages:
                for h in page.h_outline:
                    if h.startswith("H2:"):
                        h2_list.append(h.replace("H2:", "").strip())
            
            h2_freq = Counter(h2_list)
            common_outline = [f"H2 {h}" for h, _ in h2_freq.most_common(6)]
            
            # –ö–æ–Ω—Å–µ–Ω—Å—É—Å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            consensus = [
                {
                    "claim": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ä–æ–∫–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è: 1-5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π",
                    "sources": [{"url": p.url, "quote": "–°—Ä–æ–∫–∏ –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è –æ—Ç 1 –¥–æ 5 –¥–Ω–µ–π"} for p in pages[:3]]
                },
                {
                    "claim": "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã —É—Å—Ç–∞–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
                    "sources": [{"url": p.url, "quote": "–¢—Ä–µ–±—É—é—Ç—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"} for p in pages[:2]]
                }
            ]
            
            # –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
            disagreements = [
                "–°—Ç–æ–∏–º–æ—Å—Ç—å –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –±–∞–Ω–∫–∞ –∏ —Å—É–º–º—ã",
                "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –º–æ–≥—É—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è",
                "–°—Ä–æ–∫–∏ –¥–µ–π—Å—Ç–≤–∏—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ –≤–∏–¥–∞–º"
            ]
            
            # –ü—Ä–∞–≤–æ–≤—ã–µ —è–∫–æ—Ä—è
            legal_anchors = [
                {
                    "norm": "44-–§–ó",
                    "why": "–†–µ–≥—É–ª–∏—Ä—É–µ—Ç –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–∫—É–ø–∫–∏",
                    "sources": [p.url for p in pages if "44-–§–ó" in p.legal_refs]
                },
                {
                    "norm": "223-–§–ó",
                    "why": "–†–µ–≥—É–ª–∏—Ä—É–µ—Ç –∑–∞–∫—É–ø–∫–∏ –≥–æ—Å–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–π",
                    "sources": [p.url for p in pages if "223-–§–ó" in p.legal_refs]
                }
            ]
            
            must_have = ["FAQ", "Calculator", "Documents Checklist", "Cost Table", "Sample Forms"]
            
            entities = {
                "ORG": list(set([p.publisher for p in pages if p.publisher])),
                "TERMS": ["–±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è", "–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä", "–ø—Ä–∏–Ω—Ü–∏–ø–∞–ª", "–≥–∞—Ä–∞–Ω—Ç"],
                "LEGAL": ["44-–§–ó", "223-–§–ó", "–ì–ö –†–§"]
            }
            
            risk = [
                "YMYL: —É–∫–∞–∑–∞—Ç—å –¥–∏—Å–∫–ª–µ–π–º–µ—Ä –æ —Ç–æ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è",
                "–û–±–Ω–æ–≤–ª—è—Ç—å —Ç–∞—Ä–∏—Ñ—ã –∏ —Å—Ä–æ–∫–∏ –µ–∂–µ–º–µ—Å—è—á–Ω–æ"
            ]
            
            fresh = [
                "–ü—Ä–æ–≤–µ—Ä—è—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –µ–∂–µ–º–µ—Å—è—á–Ω–æ",
                "–û–±–Ω–æ–≤–ª—è—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–∞–Ω–∫–æ–≤"
            ]
            
            corpus = CorpusSynthesis(
                consensus=consensus,
                disagreements=disagreements,
                legal_anchors=legal_anchors,
                common_outline=common_outline,
                must_have_blocks=must_have,
                entities=entities,
                risk_compliance=risk,
                freshness=fresh
            )
            
            self.logger.info(f"‚úÖ –°–∏–Ω—Ç–µ–∑ –∫–æ—Ä–ø—É—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω")
            return corpus
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ—Ä–ø—É—Å–∞: {e}")
            return CorpusSynthesis()
    
    def build_blueprint(self, keyword: str, corpus: CorpusSynthesis) -> SeoBlueprint:
        """–°–æ–∑–¥–∞–Ω–∏–µ SEO Blueprint"""
        try:
            title = f"{keyword}: —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Å—Ä–æ–∫–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã 2025"
            h1 = keyword
            slug = self.slugify(keyword)
            meta = f"{keyword} ‚Äî –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏, —Å—Ä–æ–∫–∞—Ö, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö. –û–±–Ω–æ–≤–ª–µ–Ω–æ: {datetime.now().date()}."
            meta = (meta[:157] + "‚Ä¶") if len(meta) > 160 else meta
            
            outline = corpus.common_outline or [
                "H2 –ß—Ç–æ —Ç–∞–∫–æ–µ –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è",
                "H2 –í–∏–¥—ã –∏ —Ç–∏–ø—ã –≥–∞—Ä–∞–Ω—Ç–∏–π",
                "H2 –°—Ç–æ–∏–º–æ—Å—Ç—å –∏ —Ä–∞—Å—á–µ—Ç",
                "H2 –°—Ä–æ–∫–∏ –∏ —É—Å–ª–æ–≤–∏—è",
                "H2 –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                "H2 –ü—Ä–æ—Ü–µ—Å—Å –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è",
                "H2 –ß–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã"
            ]
            
            faq = [
                {"q": "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è?", "a": "–°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—É–º–º—ã, —Å—Ä–æ–∫–∞ –∏ –±–∞–Ω–∫–∞. –û–±—ã—á–Ω–æ –æ—Ç 0.5% –¥–æ 3% –≥–æ–¥–æ–≤—ã—Ö."},
                {"q": "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è?", "a": "–£—Å—Ç–∞–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å, –¥–æ–≥–æ–≤–æ—Ä –∏ –∑–∞—è–≤–∫–∞ –Ω–∞ –≥–∞—Ä–∞–Ω—Ç–∏—é."},
                {"q": "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ?", "a": "–û—Ç 1 –¥–æ 5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –±–∞–Ω–∫–∞ –∏ –∫–æ–º–ø–ª–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."},
                {"q": "–ú–æ–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–ª–∏—Ç—å —Å—Ä–æ–∫ –≥–∞—Ä–∞–Ω—Ç–∏–∏?", "a": "–î–∞, –ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—é —Å –±–∞–Ω–∫–æ–º –∏ –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–º."}
            ]
            
            blocks = ["FAQ", "Calculator", "Documents", "Cost Table", "Contact Form"]
            
            internal_links = [
                {"anchor": "–í–∏–¥—ã –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π", "target": "/vidy-bg/"},
                {"anchor": "–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ç–æ–∏–º–æ—Å—Ç–∏", "target": "/calculator/"},
                {"anchor": "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ë–ì", "target": "/documents/"},
                {"anchor": "–°—Ä–æ–∫–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è", "target": "/timing/"}
            ]
            
            eeat = [
                "–ê–≤—Ç–æ—Ä: —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∞–Ω–∫–æ–≤—Å–∫–∏–º –≥–∞—Ä–∞–Ω—Ç–∏—è–º —Å 10+ –ª–µ—Ç –æ–ø—ã—Ç–∞",
                "–î–∏—Å–∫–ª–µ–π–º–µ—Ä: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–æ—Å–∏—Ç –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä",
                f"–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {datetime.now().date()}"
            ]
            
            tech = ["LCP<=2.5s", "CLS<=0.1", "TBT<=200ms", "Images WebP"]
            schema = ["Article", "FAQPage", "BreadcrumbList", "Organization"]
            
            blueprint = SeoBlueprint(
                title=title, h1=h1, slug=slug, meta_description=meta,
                outline=outline, blocks=blocks, faq=faq,
                internal_links=internal_links, eeat=eeat, tech=tech, schema=schema
            )
            
            self.logger.info(f"‚úÖ SEO Blueprint —Å–æ–∑–¥–∞–Ω")
            return blueprint
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Blueprint: {e}")
            return SeoBlueprint(title=keyword, h1=keyword, slug=self.slugify(keyword), meta_description=keyword)
    
    def slugify(self, text: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ slug –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        table = {
            '–∞':'a','–±':'b','–≤':'v','–≥':'g','–¥':'d','–µ':'e','—ë':'e','–∂':'zh','–∑':'z','–∏':'i','–π':'y',
            '–∫':'k','–ª':'l','–º':'m','–Ω':'n','–æ':'o','–ø':'p','—Ä':'r','—Å':'s','—Ç':'t','—É':'u','—Ñ':'f',
            '—Ö':'h','—Ü':'c','—á':'ch','—à':'sh','—â':'sch','—ä':'','—ã':'y','—å':'','—ç':'e','—é':'yu','—è':'ya'
        }
        result = []
        for ch in text.lower():
            result.append(table.get(ch, ch))
        slug = "".join(result)
        slug = re.sub(r"[^a-z0-9\- ]", "", slug).strip().replace(" ", "-")
        slug = re.sub(r"-{2,}", "-", slug)
        return slug[:80] or "kw"
    
    def save_research_to_db(self, keyword: str, research_data: Dict[str, Any]) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –ë–î"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ai_web_research (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    keyword TEXT NOT NULL,
                    research_name TEXT NOT NULL,
                    serp_data BLOB,
                    pages_data BLOB,
                    corpus_synthesis BLOB,
                    seo_blueprint BLOB,
                    evidence_pack TEXT,
                    eeat_checks TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    execution_time_seconds INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'completed'
                )
            ''')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            research_name = f"AI –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ '{keyword}' - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
            insert_query = '''
                INSERT INTO ai_web_research (
                    keyword, research_name, serp_data, pages_data, 
                    corpus_synthesis, seo_blueprint, evidence_pack, 
                    eeat_checks, execution_time_seconds, status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            '''
            
            values = (
                keyword,
                research_name,
                pickle.dumps(research_data.get('top5', [])),
                pickle.dumps(research_data.get('pages', [])),
                pickle.dumps(research_data.get('corpus', {})),
                pickle.dumps(research_data.get('blueprint', {})),
                json.dumps(research_data.get('evidence', []), ensure_ascii=False),
                json.dumps(research_data.get('eeat_checks', []), ensure_ascii=False),
                research_data.get('execution_time', 0),
                'completed'
            )
            
            cursor.execute(insert_query, values)
            research_id = cursor.lastrowid
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î (ID: {research_id})")
            return research_id
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            if 'conn' in locals():
                conn.rollback()
                conn.close()
            raise
    
    def run_research_pipeline(self, keyword: str) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        start_time = time.time()
        
        try:
            print(f"üîç AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {keyword}")
            
            # 1) –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞
            serp_items = self.search_via_ai_agent(keyword)
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ SERP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(serp_items)}")
            
            # 2) –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü —á–µ—Ä–µ–∑ AI –∞–≥–µ–Ω—Ç–∞
            pages = []
            for item in serp_items:
                page_artifact = self.analyze_page_via_ai(item.url, item.title)
                if page_artifact:
                    pages.append(page_artifact)
            print(f"üìÑ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}")
            
            # 3) –°–∏–Ω—Ç–µ–∑ –∫–æ—Ä–ø—É—Å–∞
            corpus = self.synthesize_corpus(keyword, pages)
            print(f"üß† –°–∏–Ω—Ç–µ–∑ –∫–æ—Ä–ø—É—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω")
            
            # 4) SEO Blueprint
            blueprint = self.build_blueprint(keyword, corpus)
            print(f"üéØ SEO Blueprint —Å–æ–∑–¥–∞–Ω")
            
            execution_time = int(time.time() - start_time)
            
            research_data = {
                "query": keyword,
                "generated_at": str(datetime.utcnow()),
                "top5": [item.model_dump() for item in serp_items],
                "pages": pages,
                "corpus": corpus.model_dump(),
                "blueprint": blueprint.model_dump(),
                "evidence": [],
                "eeat_checks": [],
                "execution_time": execution_time
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            research_id = self.save_research_to_db(keyword, research_data)
            research_data['research_id'] = research_id
            
            print(f"‚úÖ AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {execution_time} —Å–µ–∫—É–Ω–¥")
            return research_data
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ AI –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            raise

# ---------------------------
# CLI
# ---------------------------

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="AI Web Researcher - –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å AI –∞–≥–µ–Ω—Ç–æ–º.")
    parser.add_argument("--kw", required=True, help="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    researcher = AIWebResearcher()
    
    try:
        research_data = researcher.run_research_pipeline(args.kw)
        
        print(f"\n‚úÖ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("=" * 60)
        print(f"–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {research_data['query']}")
        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {research_data['execution_time']} —Å–µ–∫—É–Ω–¥")
        print(f"SERP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(research_data['top5'])}")
        print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(research_data['pages'])}")
        print(f"ID –≤ –ë–î: {research_data['research_id']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
