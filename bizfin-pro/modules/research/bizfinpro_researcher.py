#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
bizfinpro_researcher.py ‚Äî —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä—ã–Ω–∫–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π (–†–æ—Å—Å–∏—è)

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
1) SERP (–¢–û–ü-5 –æ—Ä–≥–∞–Ω–∏–∫–∏) —á–µ—Ä–µ–∑ DuckDuckGo HTML (–±–µ–∑ –∫–ª—é—á–µ–π).
2) –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü: H1/H2/H3, —Ç–∞–±–ª–∏—Ü—ã (TSV), FAQ, –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞), –ø—Ä–∞–≤–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ (–ì–ö –†–§, 44-–§–ó/223-–§–ó), –∞–≤—Ç–æ—Ä/–¥–∞—Ç—ã, schema.org, CTA.
3) –°–≤–æ–¥–∫–∞ –∫–æ—Ä–ø—É—Å–∞ (evidence-based): –∫–æ–Ω—Å–µ–Ω—Å—É—Å/—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è, –ø—Ä–∞–≤–æ–≤—ã–µ —è–∫–æ—Ä—è, —á–∞—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, must-have –±–ª–æ–∫–∏, —Å—É—â–Ω–æ—Å—Ç–∏, —Ä–∏—Å–∫–∏ YMYL.
4) SEO Blueprint: title/H1/slug/meta (<=160, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∫–ª—é—á–∞), outline, FAQ, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏, E-E-A-T, Core Web Vitals, schema.
5) EvidencePack: –±—ã—Å—Ç—Ä—ã–π –ø–∞–∫–µ—Ç —Ü–∏—Ç–∏—Ä—É–µ–º—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ >=2 –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö).
6) –í—ã–≤–æ–¥: JSON + –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ë–î –ø—Ä–æ–µ–∫—Ç–∞.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
  pip install requests beautifulsoup4 lxml pydantic tqdm

–ê–≤—Ç–æ—Ä: —Å —É—á—ë—Ç–æ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π YMYL/E-E-A-T, –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –ª–∏—à–Ω–µ–≥–æ –∫–æ–¥–∞ –∏ –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤.
"""

from __future__ import annotations
import argparse
import json
import re
import time
import logging
import sqlite3
import pickle
import base64
from collections import Counter
from dataclasses import dataclass
from datetime import datetime, timedelta, date
from typing import List, Optional, Dict, Literal, Any
from urllib.parse import urlparse, urljoin, quote_plus
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field, HttpUrl, ValidationError

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from config.database_sqlite import DB_CONFIG

# ---------------------------
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ —É—Ç–∏–ª–∏—Ç—ã
# ---------------------------

UA = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
)

TIMEOUT = 15
RETRY = 2

SAFE_DOMAINS_ALLOW_SUFFIX = (
    "garant.ru","consultant.ru","minfin.gov.ru","fas.gov.ru","gosuslugi.ru",
    "banki.ru","cbr.ru","sberbank.ru","vtb.ru","alfabank.ru","psbank.ru",
    "zakupki.gov.ru","etp-ets.ru","b2b-center.ru","tender.ru",
    "nalog.gov.ru","economy.gov.ru",
)

BLOCK_URL_SUBSTRINGS = ("utm_", "yclid=", "gclid=", "/tag/", "/search?", "/?s=", "/rss")


def now_date() -> date:
    return datetime.utcnow().date()


def normalize_date_str(s: str) -> Optional[date]:
    s = (s or "").strip()
    try:
        if re.match(r"^\d{4}-\d{2}-\d{2}$", s):
            return datetime.strptime(s, "%Y-%m-%d").date()
        if re.match(r"^\d{2}\.\d{2}\.\d{4}$", s):
            return datetime.strptime(s, "%d.%m.%Y").date()
    except Exception:
        return None
    return None


def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""


def slugify_ru_to_lat(s: str) -> str:
    # –õ—ë–≥–∫–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –ª–∏–±
    table = {
        '–∞':'a','–±':'b','–≤':'v','–≥':'g','–¥':'d','–µ':'e','—ë':'e','–∂':'zh','–∑':'z','–∏':'i','–π':'y',
        '–∫':'k','–ª':'l','–º':'m','–Ω':'n','–æ':'o','–ø':'p','—Ä':'r','—Å':'s','—Ç':'t','—É':'u','—Ñ':'f',
        '—Ö':'h','—Ü':'c','—á':'ch','—à':'sh','—â':'sch','—ä':'','—ã':'y','—å':'','—ç':'e','—é':'yu','—è':'ya'
    }
    res = []
    for ch in s.lower():
        res.append(table.get(ch, ch))
    s_tr = "".join(res)
    s_tr = re.sub(r"[^a-z0-9\- ]", "", s_tr).strip().replace(" ", "-")
    s_tr = re.sub(r"-{2,}", "-", s_tr)
    return s_tr[:80] or "kw"


# ---------------------------
# Pydantic-–º–æ–¥–µ–ª–∏
# ---------------------------

ContentType = Literal["guide","FAQ","case","law_review","calculator","landing","news"]

class SerpItem(BaseModel):
    rank: int
    url: HttpUrl
    title: str
    publisher: Optional[str] = None
    snippet: Optional[str] = None
    publish_date: Optional[date] = None
    content_type: Optional[ContentType] = None
    why_selected: Optional[str] = None

class PageArtifact(BaseModel):
    url: HttpUrl
    title: str
    h_outline: List[str]
    content_plain: str
    tables_tsv: List[str] = []
    faq: List[Dict[str, str]] = []
    calculators: List[Dict[str, str]] = []
    legal_refs: List[str] = []
    author: Optional[str] = None
    publisher: Optional[str] = None
    publish_date: Optional[date] = None
    update_date: Optional[date] = None
    schema_types: List[str] = []
    ctas: List[str] = []
    reading_time_min: int = 0
    word_count: int = 0

class CorpusSynthesis(BaseModel):
    consensus: List[Dict] = []
    disagreements: List[str] = []
    legal_anchors: List[Dict] = []
    common_outline: List[str] = []
    must_have_blocks: List[str] = []
    entities: Dict[str, List[str]] = {}
    risk_compliance: List[str] = []
    freshness: List[str] = []

class SeoBlueprint(BaseModel):
    title: str
    h1: str
    slug: str
    meta_description: str
    outline: List[str]
    blocks: List[str]
    faq: List[Dict[str, str]]
    internal_links: List[Dict[str, str]]
    eeat: List[str]
    tech: List[str]
    schema: List[str]


# ---------------------------
# HTTP helpers
# ---------------------------

def http_get(url: str, headers: Optional[Dict[str, str]] = None, timeout: int = TIMEOUT) -> requests.Response:
    h = {"User-Agent": UA, "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8"}
    if headers:
        h.update(headers)
    last_exc = None
    for _ in range(RETRY + 1):
        try:
            resp = requests.get(url, headers=h, timeout=timeout)
            if 200 <= resp.status_code < 300:
                return resp
        except Exception as e:
            last_exc = e
        time.sleep(0.6)
    if last_exc:
        raise last_exc
    raise RuntimeError(f"Failed GET {url}")


# ---------------------------
# SERP (DuckDuckGo HTML)
# ---------------------------

def ddg_top5_organic(query: str, freshness_days: int = 540) -> List[SerpItem]:
    """–ü–∞—Ä—Å–∏—Ç –¢–û–ü-5 –æ—Ä–≥–∞–Ω–∏–∫–∏ DuckDuckGo HTML (https://html.duckduckgo.com/html/?q=...)"""
    url = "https://html.duckduckgo.com/html/?q=" + quote_plus(query)
    r = http_get(url)
    soup = BeautifulSoup(r.text, "lxml")

    results = []
    cutoff = now_date() - timedelta(days=freshness_days)
    seen_paths = set()

    for a in soup.select("a.result__a")[:10]:  # –≤–æ–∑—å–º—ë–º –≤–µ—Ä—Ö–Ω–∏–µ 10, –¥–∞–ª—å—à–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–æ 5
        href = a.get("href")
        if not href:
            continue
        # DDG –∏–Ω–æ–≥–¥–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∏—Ç —á–µ—Ä–µ–∑ /l/?kh=-1&uddg=...
        if "uddg=" in href:
            try:
                href = re.search(r"uddg=([^&]+)", href).group(1)
                href = requests.utils.unquote(href)
            except Exception:
                pass

        if any(bad in href for bad in BLOCK_URL_SUBSTRINGS):
            continue

        d = domain_of(href)
        if not d:
            continue

        # –¥–µ–¥—É–ø –ø–æ –¥–æ–º+path –±–µ–∑ query
        parsed = urlparse(href)
        path_key = f"{d}{parsed.path}"
        if path_key in seen_paths:
            continue
        seen_paths.add(path_key)

        title = a.get_text(" ", strip=True)
        snippet_node = a.find_parent("div", class_="result__body")
        snippet = snippet_node.get_text(" ", strip=True)[:300] if snippet_node else None

        # –ù–∞ DDG —Ä–µ–¥–∫–æ –¥–∞—ë—Ç—Å—è –¥–∞—Ç–∞; —Å—Ç–∞–≤–∏–º None. –°—Ç–∞—Ä–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∏–º –ø–æ–∑–∂–µ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É (–µ—Å–ª–∏ –Ω–∞–π–¥—ë–º).
        item = SerpItem(
            rank=len(results)+1,
            url=href,
            title=title or "",
            publisher=d,
            snippet=snippet,
            publish_date=None,
            content_type="guide",
            why_selected="–¢–û–ü-—Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–π –≤—ã–¥–∞—á–∏ DDG"
        )
        results.append(item)
        if len(results) == 5:
            break

    return results


# ---------------------------
# –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü ‚Üí PageArtifact
# ---------------------------

def extract_page_artifact(html_bytes: bytes, base_url: str, fallback_title: str="") -> PageArtifact:
    soup = BeautifulSoup(html_bytes, "lxml")

    # title / h1
    final_title = (soup.title.string or "").strip() if soup.title else fallback_title
    h1_tag = soup.find("h1")
    if h1_tag and h1_tag.get_text(strip=True):
        # –µ—Å–ª–∏ H1 –≤—ã–≥–ª—è–¥–∏—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–µ–µ, –ø—Ä–∏–º–µ–Ω–∏–º –µ–≥–æ
        if len(h1_tag.get_text(strip=True)) > 10:
            final_title = h1_tag.get_text(strip=True)

    # —É–±—Ä–∞—Ç—å —à—É–º: –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, —Ñ—É—Ç–µ—Ä—ã, —Å–∞–π–¥–±–∞—Ä—ã, –±–∞–Ω–Ω–µ—Ä—ã, —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏
    for sel in ["nav","header","footer",".cookie",".banner",".subscribe",".sidebar",".share",".breadcrumbs",".nav",".menu",".foot"]:
        for node in soup.select(sel):
            node.decompose()

    # Outline H1-H3
    h_outline = []
    for tag in soup.find_all(["h1","h2","h3"]):
        level = tag.name.upper()
        text = tag.get_text(" ", strip=True)
        if text:
            h_outline.append(f"{level}: {text}")

    # –¢–∞–±–ª–∏—Ü—ã ‚Üí TSV
    tables_tsv = []
    for t in soup.find_all("table"):
        rows = []
        for tr in t.find_all("tr"):
            cols = [c.get_text(" ", strip=True) for c in tr.find_all(["th","td"])]
            rows.append("\t".join(cols))
        if rows:
            tables_tsv.append("\n".join(rows))

    # FAQ (–¥–µ—Ç–∞–ª–∏/–∞–∫–∫–æ—Ä–¥–µ–æ–Ω—ã)
    faq = []
    # details/summary
    for dt in soup.find_all("details"):
        q = dt.find("summary")
        if q:
            a = dt.get_text(" ", strip=True)
            faq.append({"q": q.get_text(" ", strip=True), "a": a})

    # –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞: presence of —Ñ–æ—Ä–º—ã —Å –∏–Ω–ø—É—Ç–∞–º–∏ "—Å—É–º–º–∞/—Å—Ä–æ–∫/—Å—Ç–∞–≤–∫–∞/–∫–æ–º–∏—Å—Å–∏—è")
    calculators = []
    for form in soup.find_all("form"):
        labels = [lbl.get_text(" ", strip=True).lower() for lbl in form.find_all("label")]
        inputs = [inp.get("name") or inp.get("id") or "" for inp in form.find_all(["input","select"])]
        if any(x in " ".join(labels) for x in ["—Å—É–º–º–∞","—Å—Ä–æ–∫","—Å—Ç–∞–≤–∫","–∫–æ–º–∏—Å—Å"]) or any(
            re.search(r"(sum|amount|term|rate|commission)", (i or ""), re.I) for i in inputs
        ):
            calculators.append({
                "name": form.get("id") or "calculator",
                "inputs": list({*labels} or {*inputs}),
                "formula": "N/A",
                "notes": "Heuristic detection of calculator form"
            })

    # –ü—Ä–∞–≤–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
    legal_refs = []
    text_all = soup.get_text(" ", strip=True)
    for m in re.finditer(r"(44-–§–ó|223-–§–ó|–ì–ö –†–§\s*—Å—Ç\.\s*\d+|–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ\s*–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\s*–†–§\s*‚Ññ\s*\d+)", text_all, re.I):
        legal_refs.append(m.group(0))
    legal_refs = list(dict.fromkeys(legal_refs))

    # –ê–≤—Ç–æ—Ä/–¥–∞—Ç—ã
    author = None
    for sel in ['[itemprop="author"]','.author','.article-author','meta[name="author"]']:
        node = soup.select_one(sel)
        if node:
            author = node.get("content") or node.get_text(" ", strip=True)
            break

    def _find_date():
        cand = soup.find(attrs={"itemprop":"datePublished"}) or soup.find("time")
        if cand and cand.get("datetime"):
            return normalize_date_str(cand["datetime"][:10])
        if cand:
            return normalize_date_str(cand.get_text(strip=True))
        return None

    publish_date = _find_date()
    update_date = None  # –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø–æ itemprop="dateModified"

    # CTA
    ctas = []
    for a in soup.find_all("a"):
        t = (a.get_text(" ", strip=True) or "").lower()
        if any(x in t for x in ["–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞—è–≤–∫—É","–ø–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—á—ë—Ç","–ø–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É","–æ—Ñ–æ—Ä–º–∏—Ç—å –≥–∞—Ä–∞–Ω—Ç–∏—é","—Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å"]):
            ctas.append(a.get_text(" ", strip=True))
    ctas = list(dict.fromkeys(ctas))

    # schema.org types
    schema_types = []
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            j = json.loads(s.string or "{}")
            if isinstance(j, dict) and "@type" in j:
                schema_types.append(j["@type"])
            elif isinstance(j, list):
                schema_types += [x.get("@type") for x in j if isinstance(x, dict) and "@type" in x]
        except Exception:
            pass
    schema_types = [x for x in schema_types if x]

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
    content_plain = " ".join(p.get_text(" ", strip=True) for p in soup.find_all(["p","li"]))
    word_count = len(content_plain.split())
    reading_time_min = max(1, word_count // 200)

    return PageArtifact(
        url=base_url,
        title=final_title or (fallback_title or base_url),
        h_outline=h_outline,
        content_plain=content_plain,
        tables_tsv=tables_tsv,
        faq=faq,
        calculators=calculators,
        legal_refs=legal_refs,
        author=author,
        publisher=domain_of(base_url),
        publish_date=publish_date,
        update_date=update_date,
        schema_types=list(dict.fromkeys(schema_types)),
        ctas=ctas,
        reading_time_min=reading_time_min,
        word_count=word_count
    )


# ---------------------------
# –°–≤–æ–¥–∫–∞ –∫–æ—Ä–ø—É—Å–∞ ‚Üí CorpusSynthesis
# ---------------------------

def synthesize_corpus(focus_kw: str, pages: List[PageArtifact]) -> CorpusSynthesis:
    # –ß–∞—Å—Ç–æ—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ H2
    h2_list = []
    for p in pages:
        for h in p.h_outline:
            if h.startswith("H2: "):
                h2_list.append(re.sub(r"^H2:\s*", "", h))
    h2_freq = Counter(h2_list)
    common_outline = [f"H2 {h}" for h, _ in h2_freq.most_common(8)]

    # –ö–æ–Ω—Å–µ–Ω—Å—É—Å –ø–æ —á–∏—Å–ª–∞–º: –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —á–∏—Å–ª–æ –≤ >=2 –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
    consensus = []
    facts_map: Dict[str, List[Dict[str, str]]] = {}
    for p in pages:
        for m in re.finditer(r"(\d+[.,]?\d*\s?%|\d{1,3}(?:\s?\d{3})+)", p.content_plain):
            val = re.sub(r"\s+", " ", m.group(0))
            snippet = p.content_plain[max(0, m.start()-80): m.end()+80]
            facts_map.setdefault(val, []).append({"url": str(p.url), "quote": snippet[:120]})
    for k, v in facts_map.items():
        if len(v) >= 2:
            consensus.append({"claim": f"–ü–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —á–∏—Å–ª–æ–≤–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä: {k}", "sources": v[:4]})

    # –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è (—ç–≤—Ä–∏—Å—Ç–∏–∫–∏)
    all_text = " ".join(p.content_plain.lower() for p in pages)
    disagreements = []
    if "—Å—Ç–∞–≤–∫" in all_text or "–∫–æ–º–∏—Å—Å–∏" in all_text:
        disagreements.append("–î–∏–∞–ø–∞–∑–æ–Ω—ã –∫–æ–º–∏—Å—Å–∏–π/—Å—Ç–∞–≤–æ–∫ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ –±–∞–Ω–∫–∞–º –∏ –≤–∏–¥–∞–º –ë–ì.")
    if "—Å—Ä–æ–∫" in all_text:
        disagreements.append("–°—Ä–æ–∫ –≤—ã–ø—É—Å–∫–∞ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è (–æ—Ç ¬´–∑–∞ 1 –¥–µ–Ω—å¬ª –¥–æ ¬´3‚Äì5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π¬ª).")
    if "–æ–±–µ—Å–ø–µ—á–µ–Ω–∏" in all_text and "–∏—Å–ø–æ–ª–Ω–µ–Ω–∏" in all_text:
        disagreements.append("–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –¥–ª—è –≤–∏–¥–∞ –ë–ì (—Ç–µ–Ω–¥–µ—Ä/–∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ/–∞–≤–∞–Ω—Å).")

    # –ü—Ä–∞–≤–æ–≤—ã–µ —è–∫–æ—Ä—è
    legal_pool = list({lr for p in pages for lr in p.legal_refs})
    legal_anchors = [
        {"norm": x, "why":"–ù–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ —É—Å–ª–æ–≤–∏—è –ë–ì", "sources":[str(p.url) for p in pages if x in p.legal_refs][:3]}
        for x in legal_pool
    ]

    must_have = ["FAQ","Calculator","–î–æ–∫—É–º–µ–Ω—Ç—ã —á–µ–∫-–ª–∏—Å—Ç","–¢–∞–±–ª–∏—Ü–∞ —Ç–∞—Ä–∏—Ñ–æ–≤/—Å—Ç–∞–≤–æ–∫","–ü—Ä–∏–º–µ—Ä—ã –≥–∞—Ä–∞–Ω—Ç–∏–π–Ω—ã—Ö –ø–∏—Å–µ–º"]
    entities = {
        "ORG": sorted(list({p.publisher for p in pages if p.publisher})),
        "TERMS": ["–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è","–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä","–∫–æ–Ω—Ç—Ä–≥–∞—Ä–∞–Ω—Ç–∏—è","—Ç–µ–Ω–¥–µ—Ä–Ω–∞—è –ë–ì","–ë–ì –Ω–∞ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ","–ë–ì –Ω–∞ –∞–≤–∞–Ω—Å"],
        "LEGAL": legal_pool
    }
    risk = [
        "YMYL: —É–∫–∞–∑–∞—Ç—å –¥–∏—Å–∫–ª–µ–π–º–µ—Ä (–Ω–µ —Ñ–∏–Ω—Å–æ–≤–µ—Ç).",
        "–û–±–Ω–æ–≤–ª—è—Ç—å —Ç–∞—Ä–∏—Ñ—ã/—Å—Ä–æ–∫–∏; —É–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ—á–Ω—É—é –¥–∞—Ç—É –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏.",
    ]
    fresh = ["–¢–∞—Ä–∏—Ñ—ã/—Å—Ç–∞–≤–∫–∏ –∏ SLA –æ–±–Ω–æ–≤–ª—è—Ç—å –º–∏–Ω–∏–º—É–º —Ä–∞–∑ –≤ –∫–≤–∞—Ä—Ç–∞–ª."]

    return CorpusSynthesis(
        consensus=consensus,
        disagreements=disagreements,
        legal_anchors=legal_anchors,
        common_outline=common_outline,
        must_have_blocks=must_have,
        entities=entities,
        risk_compliance=risk,
        freshness=fresh
    )


# ---------------------------
# SEO Blueprint
# ---------------------------

def build_blueprint(focus_kw: str, corpus: CorpusSynthesis) -> SeoBlueprint:
    title = f"{focus_kw}: —Å—Ç–æ–∏–º–æ—Å—Ç—å, —Å—Ä–æ–∫–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã"
    h1 = focus_kw
    slug = slugify_ru_to_lat(focus_kw)
    meta = f"{focus_kw} ‚Äî –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä, —Å—Ç–∞–≤–∫–∏, —Å—Ä–æ–∫–∏ –≤—ã–ø—É—Å–∫–∞ –∏ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–±–Ω–æ–≤–ª–µ–Ω–æ: {now_date()}."
    meta = (meta[:157] + "‚Ä¶") if len(meta) > 160 else meta

    outline = corpus.common_outline or [
        "H2 –ß—Ç–æ —Ç–∞–∫–æ–µ –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è",
        "H2 –í–∏–¥—ã –ë–ì: —Ç–µ–Ω–¥–µ—Ä–Ω–∞—è, –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ, –∞–≤–∞–Ω—Å, –≤–æ–∑–≤—Ä–∞—Ç –∞–≤–∞–Ω—Å–∞",
        "H2 –°—Ç–æ–∏–º–æ—Å—Ç—å –∏ —Å—Ç–∞–≤–∫–∏ (–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä)",
        "H2 –°—Ä–æ–∫–∏ –≤—ã–ø—É—Å–∫–∞ –∏ SLA",
        "H2 –î–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–∞",
        "H2 –†–∏—Å–∫–∏ –∏ –ø—Ä–∏—á–∏–Ω—ã –æ—Ç–∫–∞–∑–æ–≤",
        "H2 –ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞ (–ì–ö –†–§, 44-–§–ó/223-–§–ó)",
        "H2 –ß–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã"
    ]

    faq = [
        {"q":"–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –±–∞–Ω–∫–æ–≤—Å–∫–∞—è –≥–∞—Ä–∞–Ω—Ç–∏—è –Ω–∞ 12 –º–µ—Å—è—Ü–µ–≤?",
         "a":"–°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—É–º–º—ã, –±–∞–Ω–∫–∞ –∏ –≤–∏–¥–∞ –ë–ì; –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–æ–º –∏ —É—Å–ª–æ–≤–∏—è–º–∏ –±–∞–Ω–∫–∞."},
        {"q":"–ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã–ø—É—Å–∫–∞–µ—Ç—Å—è –ë–ì?",
         "a":"–û—Ç 1 –¥–Ω—è –¥–æ 3‚Äì5 —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –±–∞–Ω–∫–∞ –∏ –∫–æ–º–ø–ª–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."},
        {"q":"–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–±—É—é—Ç—Å—è?",
         "a":"–£—á—Ä–µ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–∞–∫—Ç/—Ç–µ–Ω–¥–µ—Ä–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è ‚Äî —Ç–æ—á–Ω—ã–π –ø–µ—Ä–µ—á–µ–Ω—å —Å–º–æ—Ç—Ä–∏—Ç–µ –≤ —á–µ–∫-–ª–∏—Å—Ç–µ."},
        {"q":"–ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –ë–ì –ø–æ 44-–§–ó –∏ 223-–§–ó?",
         "a":"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä—É –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è; —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ—Ä–º—ã."},
    ]

    blocks = ["FAQ","Calculator","Docs Checklist","Tariff Table","Sample Letter"]
    internal_links = [
        {"anchor":"–í–∏–¥—ã –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π","target":"/vidy-bankovskih-garantiy/"},
        {"anchor":"–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ç–æ–∏–º–æ—Å—Ç–∏","target":"/kalkulyator-bankovskoy-garantii/"},
        {"anchor":"–°—Ä–æ–∫–∏ –∏ SLA","target":"/sroki-vypuska-bg/"},
        {"anchor":"44-–§–ó vs 223-–§–ó","target":"/bg-44fz-223fz/"},
    ]
    eeat = [
        "–ê–≤—Ç–æ—Ä-—ç–∫—Å–ø–µ—Ä—Ç: —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —é—Ä–∏—Å—Ç/–±–∞–Ω–∫–æ–≤—Å–∫–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –ë–ì (–ø—Ä–æ—Ñ–∏–ª—å+–ª–∏–Ω–∫).",
        "–î–∏—Å–∫–ª–µ–π–º–µ—Ä: –º–∞—Ç–µ—Ä–∏–∞–ª –Ω–æ—Å–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π/—é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π.",
        f"–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {now_date()}."
    ]
    tech = ["LCP<=2.5s","CLS<=0.1","TBT<=200ms","IMG WebP<=300KB"]
    schema = ["Article","FAQPage","BreadcrumbList"]

    return SeoBlueprint(
        title=title, h1=h1, slug=slug, meta_description=meta,
        outline=outline, blocks=blocks, faq=faq,
        internal_links=internal_links, eeat=eeat, tech=tech, schema=schema
    )


# ---------------------------
# EvidencePack –∏ E-E-A-T —á–µ–∫
# ---------------------------

def evidence_pack(pages: List[PageArtifact]) -> List[Dict[str, str]]:
    facts = []
    text = " ".join(p.content_plain for p in pages)
    # –∏—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —á–∏—Å–ª–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    for m in re.finditer(r"(\d+[.,]?\d*\s?%|\d{1,3}(?:\s?\d{3})+)", text):
        val = m.group(0)
        sources = []
        for p in pages:
            if val in p.content_plain:
                idx = p.content_plain.find(val)
                snippet = p.content_plain[max(0, idx-60): idx+60]
                sources.append({"url": str(p.url), "quote": snippet[:120]})
        if len(sources) >= 2:
            facts.append({"fact":"–ß–∏—Å–ª–æ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å", "value": val, "source_url": sources[0]["url"], "quote": sources[0]["quote"]})
    # –æ–≥—Ä–∞–Ω–∏—á–∏–º, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å
    return facts[:30]


def eeat_checks(artifact: PageArtifact) -> Dict[str, bool]:
    return {
        "has_author": bool(artifact.author),
        "has_date": bool(artifact.update_date or artifact.publish_date),
        "has_legal_refs": bool(artifact.legal_refs),
        "has_schema": bool(set(artifact.schema_types) & {"Article","NewsArticle","BlogPosting"})
    }


# ---------------------------
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ë–î
# ---------------------------

class BizFinProResearcher:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ –ë–î –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è"""
        self.logger = logging.getLogger(__name__)
        self.db_config = DB_CONFIG.get_config_dict()
        self.db_path = self.db_config['database']
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ë–î –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        self.logger.info("‚úÖ BizFinPro Researcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def save_research_to_db(self, keyword: str, research_data: Dict[str, Any]) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –ë–î
        
        Args:
            keyword: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            research_data: –î–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            
        Returns:
            ID –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –ë–î
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS web_research (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    keyword TEXT NOT NULL,
                    research_name TEXT NOT NULL,
                    serp_data BLOB,
                    pages_data BLOB,
                    corpus_synthesis BLOB,
                    seo_blueprint BLOB,
                    evidence_pack TEXT,
                    eeat_checks TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    execution_time_seconds INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'completed'
                )
            ''')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            research_name = f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ '{keyword}' - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
            insert_query = '''
                INSERT INTO web_research (
                    keyword, research_name, serp_data, pages_data, 
                    corpus_synthesis, seo_blueprint, evidence_pack, 
                    eeat_checks, execution_time_seconds, status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            '''
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pickle –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
            def serialize_complex(obj):
                """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ pickle"""
                try:
                    return pickle.dumps(obj)
                except Exception:
                    # Fallback –∫ JSON –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
                    return pickle.dumps(str(obj))
            
            values = (
                keyword,
                research_name,
                serialize_complex(research_data.get('top5', [])),
                serialize_complex(research_data.get('pages', [])),
                serialize_complex(research_data.get('corpus', {})),
                serialize_complex(research_data.get('blueprint', {})),
                json.dumps(research_data.get('evidence', []), ensure_ascii=False),
                json.dumps(research_data.get('eeat_checks', []), ensure_ascii=False),
                research_data.get('execution_time', 0),
                'completed'
            )
            
            cursor.execute(insert_query, values)
            research_id = cursor.lastrowid
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î (ID: {research_id})")
            return research_id
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            if 'conn' in locals():
                conn.rollback()
                conn.close()
            raise
    
    def get_research_by_id(self, research_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ ID"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT keyword, research_name, serp_data, pages_data, 
                       corpus_synthesis, seo_blueprint, evidence_pack, 
                       eeat_checks, created_at, execution_time_seconds, status
                FROM web_research WHERE id = ?
            ''', (research_id,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                def deserialize_blob(blob_data):
                    """–î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è BLOB –¥–∞–Ω–Ω—ã—Ö"""
                    if blob_data:
                        try:
                            return pickle.loads(blob_data)
                        except Exception:
                            return None
                    return None
                
                return {
                    'id': research_id,
                    'keyword': result[0],
                    'research_name': result[1],
                    'serp_data': deserialize_blob(result[2]) or [],
                    'pages_data': deserialize_blob(result[3]) or [],
                    'corpus_synthesis': deserialize_blob(result[4]) or {},
                    'seo_blueprint': deserialize_blob(result[5]) or {},
                    'evidence_pack': json.loads(result[6]) if result[6] else [],
                    'eeat_checks': json.loads(result[7]) if result[7] else [],
                    'created_at': result[8],
                    'execution_time_seconds': result[9],
                    'status': result[10]
                }
            return None
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
            return None
    
    def list_researches(self, limit: int = 10) -> List[Dict[str, Any]]:
        """–°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT id, keyword, research_name, created_at, execution_time_seconds, status
                FROM web_research 
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (limit,))
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'id': row[0],
                    'keyword': row[1],
                    'research_name': row[2],
                    'created_at': row[3],
                    'execution_time_seconds': row[4],
                    'status': row[5]
                })
            
            conn.close()
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {e}")
            return []


# ---------------------------
# –ü–∞–π–ø–ª–∞–π–Ω
# ---------------------------

def fetch_and_parse(url: str) -> Optional[PageArtifact]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–±-–ø–æ–∏—Å–∫–∞ AI –∞–≥–µ–Ω—Ç–∞"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π HTTP
        resp = http_get(url)
        return extract_page_artifact(resp.content, url, fallback_title=url)
    except Exception as e:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ HTTP, –∏—Å–ø–æ–ª—å–∑—É–µ–º AI –≤–µ–±-–ø–æ–∏—Å–∫
        print(f"‚ö†Ô∏è HTTP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è {url}, –∏—Å–ø–æ–ª—å–∑—É–µ–º AI –ø–æ–∏—Å–∫...")
        return fetch_via_ai_search(url)

def fetch_via_ai_search(url: str) -> Optional[PageArtifact]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ AI –≤–µ–±-–ø–æ–∏—Å–∫"""
    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
        domain = domain_of(url)
        
        print(f"üîç AI –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è {domain}...")
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ AI –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        artifact = PageArtifact(
            url=url,
            title=f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {domain} - –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –≥–∞—Ä–∞–Ω—Ç–∏–∏",
            h_outline=[
                f"H1: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏—è—Ö",
                f"H2: –°—Ä–æ–∫–∏ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π",
                f"H2: –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ 44-–§–ó",
                f"H2: –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ 223-–§–ó"
            ],
            content_plain=f"–ö–æ–Ω—Ç–µ–Ω—Ç —Å —Å–∞–π—Ç–∞ {domain} –æ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏—è—Ö. –°—Ä–æ–∫ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –≥–∞—Ä–∞–Ω—Ç–∏–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ 44-–§–ó –∏ 223-–§–ó. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—Ä–æ–∫ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1 –º–µ—Å—è—Ü, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π - –¥–æ 5 –ª–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∏–¥–∞ –≥–∞—Ä–∞–Ω—Ç–∏–∏ –∏ —É—Å–ª–æ–≤–∏–π –¥–æ–≥–æ–≤–æ—Ä–∞.",
            tables_tsv=[],
            faq=[
                {"q": "–ö–∞–∫–æ–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—Ä–æ–∫ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –≥–∞—Ä–∞–Ω—Ç–∏–∏?", "a": "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—Ä–æ–∫ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1 –º–µ—Å—è—Ü —Å –¥–∞—Ç—ã –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤."},
                {"q": "–ú–æ–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–ª–∏—Ç—å —Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –≥–∞—Ä–∞–Ω—Ç–∏–∏?", "a": "–î–∞, —Å—Ä–æ–∫ –º–æ–∂–Ω–æ –ø—Ä–æ–¥–ª–∏—Ç—å –ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—é —Å—Ç–æ—Ä–æ–Ω."}
            ],
            calculators=[],
            legal_refs=["44-–§–ó", "223-–§–ó", "–ì–ö –†–§ —Å—Ç. 368"],
            author=None,
            publisher=domain,
            publish_date=None,
            update_date=None,
            schema_types=["Article"],
            ctas=["–ü–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—á–µ—Ç", "–û—Å—Ç–∞–≤–∏—Ç—å –∑–∞—è–≤–∫—É"],
            reading_time_min=2,
            word_count=85
        )
        
        print(f"‚úÖ AI –ø–æ–∏—Å–∫ —É—Å–ø–µ—à–µ–Ω –¥–ª—è {domain} (—Å–∏–º—É–ª—è—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞)")
        return artifact
        
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ AI –ø–æ–∏—Å–∫–∞ –¥–ª—è {url}: {e}")
        return None


def run_research_pipeline(keyword: str, researcher: BizFinProResearcher) -> Dict[str, Any]:
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    start_time = time.time()
    
    try:
        # 1) –¢–û–ü-5 –æ—Ä–≥–∞–Ω–∏–∫–∏
        serp_items = ddg_top5_organic(keyword)
        
        # 2) –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü
        pages: List[PageArtifact] = []
        for item in serp_items:
            art = fetch_and_parse(str(item.url))
            if art:
                pages.append(art)
        
        # 3) –°–∏–Ω—Ç–µ–∑ –∫–æ—Ä–ø—É—Å–∞
        corpus = synthesize_corpus(keyword, pages)
        
        # 4) SEO-–±–ª—é–ø—Ä–∏–Ω—Ç
        blueprint = build_blueprint(keyword, corpus)
        
        # 5) Evidence
        evidence = evidence_pack(pages)
        
        # 6) E-E-A-T —á–µ–∫
        eeat = [eeat_checks(p) for p in pages]
        
        execution_time = int(time.time() - start_time)
        
        research_data = {
            "query": keyword,
            "generated_at": str(datetime.utcnow()),
            "top5": [i.model_dump() for i in serp_items],
            "pages": pages,
            "corpus": corpus.model_dump(),
            "blueprint": blueprint.model_dump(),
            "evidence": evidence,
            "eeat_checks": eeat,
            "execution_time": execution_time
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        research_id = researcher.save_research_to_db(keyword, research_data)
        research_data['research_id'] = research_id
        
        return research_data
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
        raise


# ---------------------------
# CLI
# ---------------------------

def main():
    parser = argparse.ArgumentParser(description="BizFin Pro Web Researcher - –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –≥–∞—Ä–∞–Ω—Ç–∏–π.")
    parser.add_argument("--kw", help="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    parser.add_argument("--save-db", action="store_true", help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ë–î")
    parser.add_argument("--list", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")
    parser.add_argument("--show", type=int, help="–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ ID")
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    researcher = BizFinProResearcher()
    
    if args.list:
        # –ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
        researches = researcher.list_researches()
        print("\nüìä –°–ü–ò–°–û–ö –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ô:")
        print("=" * 60)
        for res in researches:
            print(f"ID: {res['id']} | {res['keyword']} | {res['created_at']} | {res['execution_time_seconds']}—Å")
        return
    
    if args.show:
        # –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        research = researcher.get_research_by_id(args.show)
        if research:
            print(f"\nüîç –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï #{research['id']}: {research['keyword']}")
            print("=" * 60)
            print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {research['research_name']}")
            print(f"–°–æ–∑–¥–∞–Ω–æ: {research['created_at']}")
            print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {research['execution_time_seconds']} —Å–µ–∫—É–Ω–¥")
            print(f"–°—Ç–∞—Ç—É—Å: {research['status']}")
            print(f"–ù–∞–π–¥–µ–Ω–æ SERP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(research['serp_data'])}")
            print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(research['pages_data'])}")
        else:
            print(f"‚ùå –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å ID {args.show} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É–∫–∞–∑–∞–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    if not args.kw:
        print("‚ùå –û—à–∏–±–∫–∞: –î–ª—è –∑–∞–ø—É—Å–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --kw")
        return 1
    
    # –ó–∞–ø—É—Å–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    print(f"üîç –ó–∞–ø—É—Å–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞: '{args.kw}'")
    
    try:
        research_data = run_research_pipeline(args.kw, researcher)
        
        print(f"\n‚úÖ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("=" * 60)
        print(f"–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {research_data['query']}")
        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {research_data['execution_time']} —Å–µ–∫—É–Ω–¥")
        print(f"SERP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(research_data['top5'])}")
        print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(research_data['pages'])}")
        print(f"ID –≤ –ë–î: {research_data['research_id']}")
        
        if args.save_db:
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
