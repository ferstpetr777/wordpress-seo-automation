#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from modules.research.ai_web_researcher import AIWebResearcher
import sqlite3
import pickle
import json
from config.database_sqlite import DB_CONFIG

def show_research_results(research_id: int = None):
    """–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
    db_config = DB_CONFIG.get_config_dict()
    db_path = db_config['database']
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    if research_id:
        cursor.execute('''
            SELECT id, keyword, research_name, serp_data, pages_data, 
                   corpus_synthesis, seo_blueprint, created_at, execution_time_seconds
            FROM ai_web_research 
            WHERE id = ?
        ''', (research_id,))
    else:
        cursor.execute('''
            SELECT id, keyword, research_name, serp_data, pages_data, 
                   corpus_synthesis, seo_blueprint, created_at, execution_time_seconds
            FROM ai_web_research 
            ORDER BY id DESC 
            LIMIT 1
        ''')
    
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        print('‚ùå AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã')
        return
    
    research_id, keyword, research_name, serp_data, pages_data, corpus_synthesis, seo_blueprint, created_at, execution_time = result
    
    print('üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó AI –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø')
    print('=' * 80)
    print(f'üìä –û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:')
    print(f'   ID: {research_id}')
    print(f'   –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {keyword}')
    print(f'   –ù–∞–∑–≤–∞–Ω–∏–µ: {research_name}')
    print(f'   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time} —Å–µ–∫—É–Ω–¥')
    print(f'   –°–æ–∑–¥–∞–Ω–æ: {created_at}')
    print()
    
    # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    try:
        serp_items = pickle.loads(serp_data) if serp_data else []
        print(f'üìà SERP –†–ï–ó–£–õ–¨–¢–ê–¢–´ ({len(serp_items)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤):')
        for i, item in enumerate(serp_items, 1):
            print(f'   {i}. {item.get("title", "N/A")}')
            print(f'      URL: {item.get("url", "N/A")}')
            print(f'      –ò–∑–¥–∞—Ç–µ–ª—å: {item.get("publisher", "N/A")}')
            print(f'      –û–ø–∏—Å–∞–Ω–∏–µ: {item.get("snippet", "N/A")[:100]}...')
            print()
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SERP –¥–∞–Ω–Ω—ã—Ö: {e}')
        serp_items = []
    
    try:
        pages = pickle.loads(pages_data) if pages_data else []
        print(f'üìÑ –ê–ù–ê–õ–ò–ó –°–¢–†–ê–ù–ò–¶ ({len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü):')
        for i, page in enumerate(pages, 1):
            print(f'   –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}: {page.get("title", "N/A")}')
            print(f'      URL: {page.get("url", "N/A")}')
            print(f'      –ò–∑–¥–∞—Ç–µ–ª—å: {page.get("publisher", "N/A")}')
            print(f'      –°–ª–æ–≤: {page.get("word_count", 0)}')
            print(f'      –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è: {page.get("reading_time_min", 0)} –º–∏–Ω')
            print(f'      H-–∑–∞–≥–æ–ª–æ–≤–∫–∏ ({len(page.get("h_outline", []))}):')
            for h in page.get("h_outline", [])[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f'         - {h}')
            print(f'      FAQ ({len(page.get("faq", []))} –≤–æ–ø—Ä–æ—Å–æ–≤):')
            for qa in page.get("faq", [])[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2
                print(f'         Q: {qa.get("q", "N/A")}')
                print(f'         A: {qa.get("a", "N/A")}')
            print(f'      –ü—Ä–∞–≤–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏: {page.get("legal_refs", [])}')
            print(f'      CTA: {page.get("ctas", [])}')
            print()
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {e}')
        pages = []
    
    try:
        corpus = pickle.loads(corpus_synthesis) if corpus_synthesis else {}
        print(f'üß† –ê–ù–ê–õ–ò–ó –ö–û–†–ü–£–°–ê:')
        print(f'   –ö–æ–Ω—Å–µ–Ω—Å—É—Å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(corpus.get("consensus", []))} —ç–ª–µ–º–µ–Ω—Ç–æ–≤')
        print(f'   –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è: {len(corpus.get("disagreements", []))} —ç–ª–µ–º–µ–Ω—Ç–æ–≤')
        print(f'   –ü—Ä–∞–≤–æ–≤—ã–µ —è–∫–æ—Ä—è: {len(corpus.get("legal_anchors", []))} —ç–ª–µ–º–µ–Ω—Ç–æ–≤')
        print(f'   –û–±—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {len(corpus.get("common_outline", []))} –±–ª–æ–∫–æ–≤')
        
        if corpus.get('consensus'):
            print(f'\\n   üîç –ö–û–ù–°–ï–ù–°–£–°–ù–´–ï –î–ê–ù–ù–´–ï:')
            for item in corpus['consensus'][:2]:
                print(f'   - {item.get("claim", "N/A")}')
        
        if corpus.get('disagreements'):
            print(f'\\n   ‚ö†Ô∏è –†–ê–°–•–û–ñ–î–ï–ù–ò–Ø:')
            for item in corpus['disagreements'][:2]:
                print(f'   - {item}')
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ—Ä–ø—É—Å–∞: {e}')
        corpus = {}
    
    try:
        blueprint = pickle.loads(seo_blueprint) if seo_blueprint else {}
        print(f'\\nüéØ SEO BLUEPRINT:')
        print(f'   Title: {blueprint.get("title", "N/A")}')
        print(f'   H1: {blueprint.get("h1", "N/A")}')
        print(f'   Slug: {blueprint.get("slug", "N/A")}')
        print(f'   Meta: {blueprint.get("meta_description", "N/A")[:100]}...')
        print()
        
        print(f'üìã –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –°–¢–†–£–ö–¢–£–†–ê:')
        for outline in blueprint.get('outline', [])[:5]:
            print(f'   - {outline}')
        print()
        
        print(f'‚ùì FAQ –ë–õ–û–ö–ò ({len(blueprint.get("faq", []))} –≤–æ–ø—Ä–æ—Å–æ–≤):')
        for qa in blueprint.get('faq', [])[:3]:
            print(f'   Q: {qa.get("q", "N/A")}')
            print(f'   A: {qa.get("a", "N/A")}')
            print()
        
        print(f'üîó –í–ù–£–¢–†–ï–ù–ù–ò–ï –°–°–´–õ–ö–ò:')
        for link in blueprint.get('internal_links', []):
            print(f'   - {link.get("anchor", "N/A")} ‚Üí {link.get("target", "N/A")}')
        print()
        
        print(f'‚öñÔ∏è E-E-A-T –¢–†–ï–ë–û–í–ê–ù–ò–Ø:')
        for eeat in blueprint.get('eeat', []):
            print(f'   - {eeat}')
        print()
        
        print(f'üèóÔ∏è –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:')
        print(f'   - {", ".join(blueprint.get("tech", []))}')
        print()
        
        print(f'üìä SCHEMA.ORG:')
        print(f'   - {", ".join(blueprint.get("schema", []))}')
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Blueprint: {e}')
        blueprint = {}
    
    print(f'\\n‚úÖ AI –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!')
    print(f'   üÜî ID –≤ –ë–î: {research_id}')
    print(f'   ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time} —Å–µ–∫—É–Ω–¥')
    print(f'   üìä SERP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(serp_items)}')
    print(f'   üìÑ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages)}')

def list_researches():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
    
    db_config = DB_CONFIG.get_config_dict()
    db_path = db_config['database']
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT id, keyword, research_name, created_at, execution_time_seconds, status
        FROM ai_web_research 
        ORDER BY created_at DESC 
        LIMIT 10
    ''')
    
    results = cursor.fetchall()
    conn.close()
    
    print('üìä –°–ü–ò–°–û–ö AI –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ô:')
    print('=' * 60)
    
    if results:
        for res in results:
            print(f'ID: {res[0]:2d} | {res[1]:30s} | {res[2]:20s} | {res[3]:19s} | {res[4]:2d}—Å')
    else:
        print('   –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã')

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")
    parser.add_argument("--show", type=int, help="–ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ ID")
    parser.add_argument("--list", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")
    args = parser.parse_args()
    
    if args.list:
        list_researches()
    elif args.show:
        show_research_results(args.show)
    else:
        show_research_results()  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
