#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
enhanced_auto_research.py ‚Äî –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥—Ä—É–ø–ø–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –æ—á–µ—Ä–µ–¥—å –∑–∞–¥–∞—á –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
"""

import sys
import os
import json
import sqlite3
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from config.database_sqlite import DB_CONFIG

class TaskQueue:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—á–µ—Ä–µ–¥—å—é –∑–∞–¥–∞—á"""
    
    def __init__(self):
        self.db_config = DB_CONFIG.get_config_dict()
        self.db_path = self.db_config['database']
        self.init_database()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS task_queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id TEXT UNIQUE NOT NULL,
                keyword TEXT NOT NULL,
                priority INTEGER DEFAULT 1,
                status TEXT DEFAULT 'pending',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                started_at DATETIME,
                completed_at DATETIME,
                execution_time_seconds REAL,
                error_message TEXT,
                result_data TEXT
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –≥—Ä—É–ø–ø –∑–∞–¥–∞—á
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS task_groups (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_id TEXT UNIQUE NOT NULL,
                group_name TEXT NOT NULL,
                total_tasks INTEGER DEFAULT 0,
                completed_tasks INTEGER DEFAULT 0,
                failed_tasks INTEGER DEFAULT 0,
                status TEXT DEFAULT 'pending',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                started_at DATETIME,
                completed_at DATETIME,
                total_execution_time_seconds REAL
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def create_group(self, group_name: str, keywords: List[str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã –∑–∞–¥–∞—á"""
        group_id = f"group_{int(datetime.now().timestamp())}"
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø—É
        cursor.execute('''
            INSERT INTO task_groups (group_id, group_name, total_tasks, status)
            VALUES (?, ?, ?, ?)
        ''', (group_id, group_name, len(keywords), 'pending'))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å
        for i, keyword in enumerate(keywords):
            task_id = f"{group_id}_task_{i+1}"
            cursor.execute('''
                INSERT INTO task_queue (task_id, keyword, priority, status)
                VALUES (?, ?, ?, ?)
            ''', (task_id, keyword, 1, 'pending'))
        
        conn.commit()
        conn.close()
        
        print(f"‚úÖ –ì—Ä—É–ø–ø–∞ —Å–æ–∑–¥–∞–Ω–∞: {group_id}")
        print(f"üìä –ó–∞–¥–∞—á –≤ –≥—Ä—É–ø–ø–µ: {len(keywords)}")
        return group_id
    
    def get_next_task(self) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, task_id, keyword, priority
            FROM task_queue 
            WHERE status = 'pending'
            ORDER BY priority DESC, created_at ASC
            LIMIT 1
        ''')
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            return {
                'id': result[0],
                'task_id': result[1],
                'keyword': result[2],
                'priority': result[3]
            }
        return None
    
    def start_task(self, task_id: str) -> bool:
        """–û—Ç–º–µ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ –∫–∞–∫ –≤—ã–ø–æ–ª–Ω—è—é—â–µ–π—Å—è"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE task_queue 
            SET status = 'running', started_at = CURRENT_TIMESTAMP
            WHERE task_id = ?
        ''', (task_id,))
        
        success = cursor.rowcount > 0
        conn.commit()
        conn.close()
        return success
    
    def complete_task(self, task_id: str, execution_time: float, result_data: str, error: str = None):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        status = 'completed' if not error else 'failed'
        
        cursor.execute('''
            UPDATE task_queue 
            SET status = ?, completed_at = CURRENT_TIMESTAMP, 
                execution_time_seconds = ?, result_data = ?, error_message = ?
            WHERE task_id = ?
        ''', (status, execution_time, result_data, error, task_id))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø—ã
        cursor.execute('''
            UPDATE task_groups 
            SET completed_tasks = completed_tasks + 1
            WHERE group_id = ?
        ''', (task_id.split('_task_')[0],))
        
        conn.commit()
        conn.close()
    
    def get_group_status(self, group_id: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≥—Ä—É–ø–ø—ã"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT total_tasks, completed_tasks, failed_tasks, status
            FROM task_groups WHERE group_id = ?
        ''', (group_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            return {
                'total_tasks': result[0],
                'completed_tasks': result[1],
                'failed_tasks': result[2],
                'status': result[3],
                'progress_percent': round((result[1] + result[2]) / result[0] * 100, 2)
            }
        return {}

class EnhancedWebResearcher:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏"""
    
    def __init__(self):
        self.db_config = DB_CONFIG.get_config_dict()
        self.db_path = self.db_config['database']
    
    def get_instruction_from_db(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –ë–î"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT instruction_data FROM research_instructions 
                WHERE instruction_id = "web_research_standard_2025"
            ''')
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return json.loads(result[0])
            else:
                print("‚ùå –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î!")
                return None
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: {e}")
            return None
    
    def perform_web_search_analysis(self, keyword: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏"""
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
            instruction = self.get_instruction_from_db()
            if not instruction:
                return {"error": "Instruction not found"}
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            search_queries = self.generate_search_queries(keyword, instruction)
            
            # –°–∏–º—É–ª—è—Ü–∏—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –≤—ã–∑–æ–≤ web_search())
            search_results = []
            search_start = time.time()
            
            for query in search_queries:
                query_start = time.time()
                # –ó–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π web_search(query)
                result = {
                    "query": query,
                    "results_found": True,
                    "execution_time": time.time() - query_start
                }
                search_results.append(result)
            
            search_time = time.time() - search_start
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            analysis_start = time.time()
            analysis_data = self.create_full_analysis(keyword, search_results, instruction)
            analysis_time = time.time() - analysis_start
            
            total_time = time.time() - start_time
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
            analysis_data['execution_metrics'] = {
                'total_execution_time_seconds': round(total_time, 2),
                'search_time_seconds': round(search_time, 2),
                'analysis_time_seconds': round(analysis_time, 2),
                'queries_count': len(search_queries),
                'average_query_time': round(search_time / len(search_queries), 2)
            }
            
            return analysis_data
            
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                "error": str(e),
                "execution_time_seconds": round(execution_time, 2)
            }
    
    def generate_search_queries(self, keyword: str, instruction: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        queries = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫
        primary_query = instruction['web_search_strategy']['primary_search']['query'].format(keyword=keyword)
        queries.append(primary_query)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∏—Å–∫–∏
        for search in instruction['web_search_strategy']['secondary_searches']:
            query = search['query'].format(
                keyword=keyword,
                brand_domain="example.com",
                year="2025"
            )
            queries.append(query)
        
        return queries
    
    def create_full_analysis(self, keyword: str, search_results: List[Dict], instruction: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        return {
            'keyword': keyword,
            'created_at': datetime.now().isoformat(),
            'instruction_applied': 'web_research_standard_2025',
            'search_queries': [r['query'] for r in search_results],
            'search_results_count': len(search_results),
            'intent_analysis': {'intent': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π'},
            'serp_analysis': {'total_results': len(search_results)},
            'semantic_clusters': {'primary': {'keywords': [keyword]}},
            'content_gaps': ['–ö–æ–Ω—Ç–µ–Ω—Ç-–≥—ç–ø 1', '–ö–æ–Ω—Ç–µ–Ω—Ç-–≥—ç–ø 2'],
            'facts_and_figures': [{'fact': '–§–∞–∫—Ç 1', 'source': 'example.com'}],
            'seo_elements': {'title_variants': [f'{keyword}: –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ']},
            'faq': [{'question': '–í–æ–ø—Ä–æ—Å 1', 'answer': '–û—Ç–≤–µ—Ç 1'}],
            'kpi_brief': {'goal': '–ò–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ —Ç–µ–º–µ', 'volume': '2500 —Å–ª–æ–≤'}
        }
    
    def save_to_database(self, keyword: str, analysis_data: Dict[str, Any]) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS enhanced_web_research (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    keyword TEXT NOT NULL,
                    research_data TEXT NOT NULL,
                    execution_metrics TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'completed'
                )
            ''')
            
            insert_query = '''
                INSERT INTO enhanced_web_research (keyword, research_data, execution_metrics, status)
                VALUES (?, ?, ?, ?)
            '''
            
            cursor.execute(insert_query, (
                keyword,
                json.dumps(analysis_data, ensure_ascii=False),
                json.dumps(analysis_data.get('execution_metrics', {}), ensure_ascii=False),
                'completed'
            ))
            
            research_id = cursor.lastrowid
            conn.commit()
            conn.close()
            
            return research_id
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            return None

class BatchProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–∞–∫–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á"""
    
    def __init__(self, max_workers: int = 3):
        self.queue = TaskQueue()
        self.researcher = EnhancedWebResearcher()
        self.max_workers = max_workers
    
    def process_single_keyword(self, task_info: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        task_id = task_info['task_id']
        keyword = task_info['keyword']
        
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {keyword}")
        
        # –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω—è—é—â—É—é—Å—è
        if not self.queue.start_task(task_id):
            return {"error": "Failed to start task"}
        
        start_time = time.time()
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
            analysis_data = self.researcher.perform_web_search_analysis(keyword)
            
            if "error" in analysis_data:
                execution_time = time.time() - start_time
                self.queue.complete_task(task_id, execution_time, "", analysis_data["error"])
                return {"error": analysis_data["error"], "task_id": task_id}
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            research_id = self.researcher.save_to_database(keyword, analysis_data)
            
            execution_time = time.time() - start_time
            result_data = json.dumps({
                "research_id": research_id,
                "keyword": keyword,
                "execution_time": execution_time
            })
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–¥–∞—á—É
            self.queue.complete_task(task_id, execution_time, result_data)
            
            print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {keyword} (–≤—Ä–µ–º—è: {execution_time:.2f}—Å)")
            return {"success": True, "task_id": task_id, "research_id": research_id}
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            self.queue.complete_task(task_id, execution_time, "", error_msg)
            print(f"‚ùå –û—à–∏–±–∫–∞: {keyword} - {error_msg}")
            return {"error": error_msg, "task_id": task_id}
    
    def process_group(self, group_id: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã –∑–∞–¥–∞—á"""
        print(f"üöÄ –ó–ê–ü–£–°–ö –û–ë–†–ê–ë–û–¢–ö–ò –ì–†–£–ü–ü–´: {group_id}")
        print("=" * 60)
        
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            future_to_task = {}
            
            while True:
                task = self.queue.get_next_task()
                if not task:
                    break
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∏ –∏–∑ –Ω–∞—à–µ–π –≥—Ä—É–ø–ø—ã
                if not task['task_id'].startswith(group_id):
                    continue
                
                future = executor.submit(self.process_single_keyword, task)
                future_to_task[future] = task
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                    status = self.queue.get_group_status(group_id)
                    print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {status['completed_tasks']}/{status['total_tasks']} ({status['progress_percent']}%)")
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ {task['task_id']}: {e}")
                    results.append({"error": str(e), "task_id": task['task_id']})
        
        total_time = time.time() - start_time
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø—ã
        conn = sqlite3.connect(self.queue.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE task_groups 
            SET status = 'completed', completed_at = CURRENT_TIMESTAMP, 
                total_execution_time_seconds = ?
            WHERE group_id = ?
        ''', (total_time, group_id))
        
        conn.commit()
        conn.close()
        
        return {
            "group_id": group_id,
            "total_execution_time": total_time,
            "results": results,
            "successful": len([r for r in results if "success" in r]),
            "failed": len([r for r in results if "error" in r])
        }

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if len(sys.argv) < 2:
        print("üîç –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:")
        print("python3 enhanced_auto_research.py \"–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ\"")
        print("python3 enhanced_auto_research.py --group \"–Ω–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã\" \"–∫–ª—é—á1\" \"–∫–ª—é—á2\" ...")
        print("python3 enhanced_auto_research.py --process-group \"group_id\"")
        print("python3 enhanced_auto_research.py --status \"group_id\"")
        return 1
    
    if sys.argv[1] == "--group" and len(sys.argv) > 3:
        # –ì—Ä—É–ø–ø–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        group_name = sys.argv[2]
        keywords = sys.argv[3:]
        
        print(f"üîç –°–û–ó–î–ê–ù–ò–ï –ì–†–£–ü–ü–´ –ó–ê–î–ê–ß")
        print(f"üìù –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã: {group_name}")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(keywords)}")
        print("=" * 60)
        
        queue = TaskQueue()
        group_id = queue.create_group(group_name, keywords)
        
        print(f"‚úÖ –ì—Ä—É–ø–ø–∞ —Å–æ–∑–¥–∞–Ω–∞: {group_id}")
        print(f"üöÄ –î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
        print(f"python3 enhanced_auto_research.py --process-group {group_id}")
        
        return 0
    
    elif sys.argv[1] == "--process-group" and len(sys.argv) > 2:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã
        group_id = sys.argv[2]
        
        processor = BatchProcessor(max_workers=3)
        result = processor.process_group(group_id)
        
        print(f"\n‚úÖ –ì–†–£–ü–ü–ê –û–ë–†–ê–ë–û–¢–ê–ù–ê!")
        print(f"üÜî Group ID: {result['group_id']}")
        print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {result['total_execution_time']:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {result['successful']}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {result['failed']}")
        
        return 0
    
    elif sys.argv[1] == "--status" and len(sys.argv) > 2:
        # –°—Ç–∞—Ç—É—Å –≥—Ä—É–ø–ø—ã
        group_id = sys.argv[2]
        
        queue = TaskQueue()
        status = queue.get_group_status(group_id)
        
        if status:
            print(f"üìä –°–¢–ê–¢–£–° –ì–†–£–ü–ü–´: {group_id}")
            print(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {status['completed_tasks']}/{status['total_tasks']} ({status['progress_percent']}%)")
            print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {status['completed_tasks']}")
            print(f"‚ùå –û—à–∏–±–æ–∫: {status['failed_tasks']}")
            print(f"üìã –°—Ç–∞—Ç—É—Å: {status['status']}")
        else:
            print(f"‚ùå –ì—Ä—É–ø–ø–∞ {group_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        return 0
    
    else:
        # –û–¥–∏–Ω–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        keyword = sys.argv[1]
        
        print(f"üîç –û–î–ò–ù–û–ß–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê")
        print(f"üéØ –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {keyword}")
        print("=" * 60)
        
        researcher = EnhancedWebResearcher()
        
        start_time = time.time()
        analysis_data = researcher.perform_web_search_analysis(keyword)
        total_time = time.time() - start_time
        
        if "error" in analysis_data:
            print(f"‚ùå –û—à–∏–±–∫–∞: {analysis_data['error']}")
            return 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        research_id = researcher.save_to_database(keyword, analysis_data)
        
        metrics = analysis_data.get('execution_metrics', {})
        
        print(f"\n‚úÖ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üÜî ID –≤ –ë–î: {research_id}")
        print(f"üéØ –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {keyword}")
        print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {metrics.get('total_execution_time_seconds', 0):.2f}—Å")
        print(f"üîç –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {metrics.get('search_time_seconds', 0):.2f}—Å")
        print(f"üìä –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {metrics.get('analysis_time_seconds', 0):.2f}—Å")
        print(f"üìã –ó–∞–ø—Ä–æ—Å–æ–≤: {metrics.get('queries_count', 0)}")
        
        return 0

if __name__ == "__main__":
    exit(main())
